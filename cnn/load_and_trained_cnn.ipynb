{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134cc523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 873 samples in the dataset.\n",
      "tensor([[[1.4664e-03, 2.7458e-04, 1.9349e-03,  ..., 2.8299e-03,\n",
      "          1.3179e-03, 6.2801e-04],\n",
      "         [3.8170e-04, 2.9188e-03, 1.4405e-02,  ..., 4.8504e-02,\n",
      "          1.0698e-02, 4.3954e-03],\n",
      "         [2.0566e-03, 3.4303e-02, 4.1252e-02,  ..., 9.0840e-02,\n",
      "          3.2409e-02, 1.4535e-02],\n",
      "         ...,\n",
      "         [3.1938e-06, 2.4180e-06, 2.5583e-06,  ..., 1.6207e-04,\n",
      "          1.1834e-04, 3.6929e-05],\n",
      "         [2.7774e-06, 1.6969e-06, 1.3460e-06,  ..., 1.1454e-04,\n",
      "          1.9588e-04, 6.4826e-05],\n",
      "         [3.1109e-06, 2.1604e-06, 2.2464e-06,  ..., 6.4307e-05,\n",
      "          8.7382e-05, 4.4857e-05]]], device='cuda:0')\n",
      "3\n",
      "torch.Size([1, 64, 44])\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\Jay\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\jay\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             160\n",
      "              ReLU-2           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
      "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
      "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 14]               0\n",
      "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
      "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-11           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
      "          Flatten-13                 [-1, 2560]               0\n",
      "           Linear-14                   [-1, 10]          25,610\n",
      "          Softmax-15                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 122,762\n",
      "Trainable params: 122,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.83\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from load_data_gpu import UrbanSoundDataset\n",
    "import torchaudio\n",
    "from cnn import CNNNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb8385b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9658135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858702c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.7433e-03, 1.6207e-04, 4.0781e-04,  ..., 2.0526e-03,\n",
      "          6.8678e-03, 1.7846e-02],\n",
      "         [6.4027e-04, 5.4754e-03, 2.8272e-02,  ..., 5.5625e-03,\n",
      "          8.6759e-02, 5.2350e-02],\n",
      "         [1.0592e-03, 1.2941e-02, 8.1117e-02,  ..., 5.6558e-03,\n",
      "          6.1651e-02, 1.0182e-01],\n",
      "         ...,\n",
      "         [4.8316e-06, 2.0553e-06, 2.5774e-06,  ..., 3.5377e-06,\n",
      "          3.0520e-06, 4.1328e-06],\n",
      "         [4.0104e-06, 4.2231e-06, 4.8558e-06,  ..., 1.7981e-06,\n",
      "          3.2201e-06, 6.0627e-06],\n",
      "         [4.0844e-06, 4.0532e-06, 4.6760e-06,  ..., 1.6818e-06,\n",
      "          3.3693e-06, 2.2788e-06]]], device='cuda:0')\n",
      "3\n",
      "torch.Size([1, 64, 44])\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNOTATIONS_FILE = r\"E:\\DL_audio\\with_pytorch\\data1\\metadata\\fold_1_annotation.csv\"\n",
    "AUDIO_DIR = r\"E:\\DL_audio\\with_pytorch\\data1\\audio\"\n",
    "\n",
    "SAMPLE_RATE=22050\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate = SAMPLE_RATE,\n",
    "                                              n_fft = 1024,\n",
    "                                              hop_length = 512,\n",
    "                                              n_mels = 64)\n",
    "    \n",
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE,AUDIO_DIR,mel_spec,SAMPLE_RATE,NUM_SAMPLES,device)\n",
    "\n",
    "signal , label = usd[0]\n",
    "\n",
    "print(signal) , print(label) , print(signal.shape), print(signal.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd4d32",
   "metadata": {},
   "source": [
    "train = 0.8*len(usd)\n",
    "\n",
    "train_data = usd[:train]\n",
    "test_data = usd[train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dce42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "Batch=128\n",
    "\n",
    "train_data_loader = create_data_loader(usd, batch_size=Batch)\n",
    "#test_data_loader = create_data_loader(test_data, batch_size=Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9965efde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNetwork(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loaded_model_0 = CNNNetwork().to(device)\n",
    "loaded_model_0.load_state_dict(torch.load(f=\"cnnnet.pth\"))\n",
    "print(loaded_model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8aa7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true,y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred))*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9442fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data_loader, loss_fn, accuracy_fn, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    for X,y in data_loader:\n",
    "        X , y = X.to(device) , y.to(device)\n",
    "        \n",
    "        y_logits = model(X).squeeze()\n",
    "        y_preds = torch.argmax(torch.round(torch.sigmoid(y_logits)),dim=1)\n",
    "        \n",
    "        #print(y_preds) , print(y)\n",
    "        \n",
    "        loss = loss_fn(y_logits,y)\n",
    "        acc = accuracy_fn(y,y_preds)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Loss: {loss}| Acc: {acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c949db",
   "metadata": {},
   "source": [
    "def test_step(model, data_loader, loss_fn, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for X,y in data_loader:\n",
    "            X , y = X.to(device) , y.to(device)\n",
    "\n",
    "            test_pred = model(X)\n",
    "\n",
    "            test_loss = loss_fn(test_pred,y)\n",
    "\n",
    "        \n",
    "    print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf67c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=loaded_model_0.parameters(),\n",
    "                            lr=LEARNING_RATE,\n",
    "                           weight_decay=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a363c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e53e1b03a44182913c9ecf6150d665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      "-----------\n",
      "Loss: 1.873665452003479| Acc: 41.904761904761905\n",
      "Epoch: 1 \n",
      "-----------\n",
      "Loss: 1.8747897148132324| Acc: 41.904761904761905\n",
      "Epoch: 2 \n",
      "-----------\n",
      "Loss: 1.8646284341812134| Acc: 43.80952380952381\n",
      "Epoch: 3 \n",
      "-----------\n",
      "Loss: 1.8643865585327148| Acc: 42.857142857142854\n",
      "Epoch: 4 \n",
      "-----------\n",
      "Loss: 1.8724570274353027| Acc: 40.95238095238095\n",
      "Epoch: 5 \n",
      "-----------\n",
      "Loss: 1.9007394313812256| Acc: 31.428571428571427\n",
      "Epoch: 6 \n",
      "-----------\n",
      "Loss: 1.865067958831787| Acc: 36.19047619047619\n",
      "Epoch: 7 \n",
      "-----------\n",
      "Loss: 1.846276879310608| Acc: 40.95238095238095\n",
      "Epoch: 8 \n",
      "-----------\n",
      "Loss: 1.8491833209991455| Acc: 37.142857142857146\n",
      "Epoch: 9 \n",
      "-----------\n",
      "Loss: 1.8456891775131226| Acc: 38.095238095238095\n",
      "Epoch: 10 \n",
      "-----------\n",
      "Loss: 1.8534879684448242| Acc: 36.19047619047619\n",
      "Epoch: 11 \n",
      "-----------\n",
      "Loss: 1.8473864793777466| Acc: 41.904761904761905\n",
      "Epoch: 12 \n",
      "-----------\n",
      "Loss: 1.8478716611862183| Acc: 41.904761904761905\n",
      "Epoch: 13 \n",
      "-----------\n",
      "Loss: 1.8466911315917969| Acc: 40.95238095238095\n",
      "Epoch: 14 \n",
      "-----------\n",
      "Loss: 1.849819540977478| Acc: 35.23809523809524\n",
      "Epoch: 15 \n",
      "-----------\n",
      "Loss: 1.8463014364242554| Acc: 41.904761904761905\n",
      "Epoch: 16 \n",
      "-----------\n",
      "Loss: 1.84884512424469| Acc: 40.0\n",
      "Epoch: 17 \n",
      "-----------\n",
      "Loss: 1.8471232652664185| Acc: 36.19047619047619\n",
      "Epoch: 18 \n",
      "-----------\n",
      "Loss: 1.8470133543014526| Acc: 35.23809523809524\n",
      "Epoch: 19 \n",
      "-----------\n",
      "Loss: 1.8470094203948975| Acc: 35.23809523809524\n",
      "Epoch: 20 \n",
      "-----------\n",
      "Loss: 1.8456565141677856| Acc: 35.23809523809524\n",
      "Epoch: 21 \n",
      "-----------\n",
      "Loss: 1.8468451499938965| Acc: 35.23809523809524\n",
      "Epoch: 22 \n",
      "-----------\n",
      "Loss: 1.8464291095733643| Acc: 35.23809523809524\n",
      "Epoch: 23 \n",
      "-----------\n",
      "Loss: 1.8478586673736572| Acc: 35.23809523809524\n",
      "Epoch: 24 \n",
      "-----------\n",
      "Loss: 1.8461852073669434| Acc: 35.23809523809524\n",
      "Epoch: 25 \n",
      "-----------\n",
      "Loss: 1.927190899848938| Acc: 30.476190476190478\n",
      "Epoch: 26 \n",
      "-----------\n",
      "Loss: 1.8992681503295898| Acc: 33.33333333333333\n",
      "Epoch: 27 \n",
      "-----------\n",
      "Loss: 1.844208836555481| Acc: 39.04761904761905\n",
      "Epoch: 28 \n",
      "-----------\n",
      "Loss: 1.8456708192825317| Acc: 40.95238095238095\n",
      "Epoch: 29 \n",
      "-----------\n",
      "Loss: 1.846415400505066| Acc: 37.142857142857146\n",
      "Epoch: 30 \n",
      "-----------\n",
      "Loss: 1.9082629680633545| Acc: 32.38095238095238\n",
      "Epoch: 31 \n",
      "-----------\n",
      "Loss: 1.8513214588165283| Acc: 35.23809523809524\n",
      "Epoch: 32 \n",
      "-----------\n",
      "Loss: 1.9702028036117554| Acc: 29.523809523809526\n",
      "Epoch: 33 \n",
      "-----------\n",
      "Loss: 1.9474577903747559| Acc: 31.428571428571427\n",
      "Epoch: 34 \n",
      "-----------\n",
      "Loss: 1.8550946712493896| Acc: 34.285714285714285\n",
      "Epoch: 35 \n",
      "-----------\n",
      "Loss: 1.8995164632797241| Acc: 33.33333333333333\n",
      "Epoch: 36 \n",
      "-----------\n",
      "Loss: 1.8941067457199097| Acc: 37.142857142857146\n",
      "Epoch: 37 \n",
      "-----------\n",
      "Loss: 1.904951810836792| Acc: 40.0\n",
      "Epoch: 38 \n",
      "-----------\n",
      "Loss: 1.861947774887085| Acc: 42.857142857142854\n",
      "Epoch: 39 \n",
      "-----------\n",
      "Loss: 1.8687875270843506| Acc: 43.80952380952381\n",
      "Epoch: 40 \n",
      "-----------\n",
      "Loss: 1.8756204843521118| Acc: 33.33333333333333\n",
      "Epoch: 41 \n",
      "-----------\n",
      "Loss: 1.8737752437591553| Acc: 32.38095238095238\n",
      "Epoch: 42 \n",
      "-----------\n",
      "Loss: 1.8613100051879883| Acc: 43.80952380952381\n",
      "Epoch: 43 \n",
      "-----------\n",
      "Loss: 1.8561913967132568| Acc: 41.904761904761905\n",
      "Epoch: 44 \n",
      "-----------\n",
      "Loss: 1.86146080493927| Acc: 45.714285714285715\n",
      "Epoch: 45 \n",
      "-----------\n",
      "Loss: 1.859995722770691| Acc: 41.904761904761905\n",
      "Epoch: 46 \n",
      "-----------\n",
      "Loss: 1.8494648933410645| Acc: 39.04761904761905\n",
      "Epoch: 47 \n",
      "-----------\n",
      "Loss: 1.8449081182479858| Acc: 40.0\n",
      "Epoch: 48 \n",
      "-----------\n",
      "Loss: 1.844878911972046| Acc: 39.04761904761905\n",
      "Epoch: 49 \n",
      "-----------\n",
      "Loss: 1.8444092273712158| Acc: 41.904761904761905\n",
      "Epoch: 50 \n",
      "-----------\n",
      "Loss: 1.8447068929672241| Acc: 40.95238095238095\n",
      "Epoch: 51 \n",
      "-----------\n",
      "Loss: 1.84469473361969| Acc: 41.904761904761905\n",
      "Epoch: 52 \n",
      "-----------\n",
      "Loss: 1.844602108001709| Acc: 39.04761904761905\n",
      "Epoch: 53 \n",
      "-----------\n",
      "Loss: 1.8446699380874634| Acc: 39.04761904761905\n",
      "Epoch: 54 \n",
      "-----------\n",
      "Loss: 1.8445714712142944| Acc: 38.095238095238095\n",
      "Epoch: 55 \n",
      "-----------\n",
      "Loss: 1.84480619430542| Acc: 40.95238095238095\n",
      "Epoch: 56 \n",
      "-----------\n",
      "Loss: 1.8450558185577393| Acc: 40.0\n",
      "Epoch: 57 \n",
      "-----------\n",
      "Loss: 1.8440461158752441| Acc: 40.0\n",
      "Epoch: 58 \n",
      "-----------\n",
      "Loss: 1.8439933061599731| Acc: 40.0\n",
      "Epoch: 59 \n",
      "-----------\n",
      "Loss: 1.8563861846923828| Acc: 36.19047619047619\n",
      "Epoch: 60 \n",
      "-----------\n",
      "Loss: 1.8525232076644897| Acc: 38.095238095238095\n",
      "Epoch: 61 \n",
      "-----------\n",
      "Loss: 1.8425836563110352| Acc: 40.0\n",
      "Epoch: 62 \n",
      "-----------\n",
      "Loss: 1.855588436126709| Acc: 38.095238095238095\n",
      "Epoch: 63 \n",
      "-----------\n",
      "Loss: 1.8880183696746826| Acc: 34.285714285714285\n",
      "Epoch: 64 \n",
      "-----------\n",
      "Loss: 1.8508086204528809| Acc: 40.95238095238095\n",
      "Epoch: 65 \n",
      "-----------\n",
      "Loss: 1.8474795818328857| Acc: 41.904761904761905\n",
      "Epoch: 66 \n",
      "-----------\n",
      "Loss: 1.843222975730896| Acc: 42.857142857142854\n",
      "Epoch: 67 \n",
      "-----------\n",
      "Loss: 1.842900276184082| Acc: 40.95238095238095\n",
      "Epoch: 68 \n",
      "-----------\n",
      "Loss: 1.841496229171753| Acc: 42.857142857142854\n",
      "Epoch: 69 \n",
      "-----------\n",
      "Loss: 1.8424664735794067| Acc: 41.904761904761905\n",
      "Epoch: 70 \n",
      "-----------\n",
      "Loss: 1.9176515340805054| Acc: 35.23809523809524\n",
      "Epoch: 71 \n",
      "-----------\n",
      "Loss: 1.8424876928329468| Acc: 43.80952380952381\n",
      "Epoch: 72 \n",
      "-----------\n",
      "Loss: 1.8405961990356445| Acc: 41.904761904761905\n",
      "Epoch: 73 \n",
      "-----------\n",
      "Loss: 1.8394356966018677| Acc: 42.857142857142854\n",
      "Epoch: 74 \n",
      "-----------\n",
      "Loss: 1.8407241106033325| Acc: 40.95238095238095\n",
      "Epoch: 75 \n",
      "-----------\n",
      "Loss: 1.8414865732192993| Acc: 39.04761904761905\n",
      "Epoch: 76 \n",
      "-----------\n",
      "Loss: 1.8414424657821655| Acc: 38.095238095238095\n",
      "Epoch: 77 \n",
      "-----------\n",
      "Loss: 1.8417093753814697| Acc: 38.095238095238095\n",
      "Epoch: 78 \n",
      "-----------\n",
      "Loss: 1.8415826559066772| Acc: 38.095238095238095\n",
      "Epoch: 79 \n",
      "-----------\n",
      "Loss: 1.841432809829712| Acc: 39.04761904761905\n",
      "Epoch: 80 \n",
      "-----------\n",
      "Loss: 1.8412786722183228| Acc: 38.095238095238095\n",
      "Epoch: 81 \n",
      "-----------\n",
      "Loss: 1.8411195278167725| Acc: 38.095238095238095\n",
      "Epoch: 82 \n",
      "-----------\n",
      "Loss: 1.841153621673584| Acc: 39.04761904761905\n",
      "Epoch: 83 \n",
      "-----------\n",
      "Loss: 1.8411099910736084| Acc: 39.04761904761905\n",
      "Epoch: 84 \n",
      "-----------\n",
      "Loss: 1.8408993482589722| Acc: 38.095238095238095\n",
      "Epoch: 85 \n",
      "-----------\n",
      "Loss: 1.8414711952209473| Acc: 38.095238095238095\n",
      "Epoch: 86 \n",
      "-----------\n",
      "Loss: 1.84093177318573| Acc: 38.095238095238095\n",
      "Epoch: 87 \n",
      "-----------\n",
      "Loss: 1.8411263227462769| Acc: 38.095238095238095\n",
      "Epoch: 88 \n",
      "-----------\n",
      "Loss: 1.8408057689666748| Acc: 38.095238095238095\n",
      "Epoch: 89 \n",
      "-----------\n",
      "Loss: 1.8403685092926025| Acc: 38.095238095238095\n",
      "Epoch: 90 \n",
      "-----------\n",
      "Loss: 1.8402234315872192| Acc: 38.095238095238095\n",
      "Epoch: 91 \n",
      "-----------\n",
      "Loss: 1.840475082397461| Acc: 38.095238095238095\n",
      "Epoch: 92 \n",
      "-----------\n",
      "Loss: 1.8403209447860718| Acc: 38.095238095238095\n",
      "Epoch: 93 \n",
      "-----------\n",
      "Loss: 1.8402656316757202| Acc: 38.095238095238095\n",
      "Epoch: 94 \n",
      "-----------\n",
      "Loss: 1.840138554573059| Acc: 39.04761904761905\n",
      "Epoch: 95 \n",
      "-----------\n",
      "Loss: 1.840138554573059| Acc: 38.095238095238095\n",
      "Epoch: 96 \n",
      "-----------\n",
      "Loss: 1.8399083614349365| Acc: 38.095238095238095\n",
      "Epoch: 97 \n",
      "-----------\n",
      "Loss: 1.8396873474121094| Acc: 39.04761904761905\n",
      "Epoch: 98 \n",
      "-----------\n",
      "Loss: 1.8399425745010376| Acc: 39.04761904761905\n",
      "Epoch: 99 \n",
      "-----------\n",
      "Loss: 1.8395346403121948| Acc: 39.04761904761905\n",
      "Epoch: 100 \n",
      "-----------\n",
      "Loss: 1.8397928476333618| Acc: 38.095238095238095\n",
      "Epoch: 101 \n",
      "-----------\n",
      "Loss: 1.83978271484375| Acc: 38.095238095238095\n",
      "Epoch: 102 \n",
      "-----------\n",
      "Loss: 1.8392610549926758| Acc: 37.142857142857146\n",
      "Epoch: 103 \n",
      "-----------\n",
      "Loss: 1.8396986722946167| Acc: 36.19047619047619\n",
      "Epoch: 104 \n",
      "-----------\n",
      "Loss: 1.839463710784912| Acc: 38.095238095238095\n",
      "Epoch: 105 \n",
      "-----------\n",
      "Loss: 1.839300513267517| Acc: 38.095238095238095\n",
      "Epoch: 106 \n",
      "-----------\n",
      "Loss: 1.8394060134887695| Acc: 39.04761904761905\n",
      "Epoch: 107 \n",
      "-----------\n",
      "Loss: 1.8392611742019653| Acc: 38.095238095238095\n",
      "Epoch: 108 \n",
      "-----------\n",
      "Loss: 1.8393118381500244| Acc: 37.142857142857146\n",
      "Epoch: 109 \n",
      "-----------\n",
      "Loss: 1.8390169143676758| Acc: 37.142857142857146\n",
      "Epoch: 110 \n",
      "-----------\n",
      "Loss: 1.8385262489318848| Acc: 37.142857142857146\n",
      "Epoch: 111 \n",
      "-----------\n",
      "Loss: 1.8391259908676147| Acc: 37.142857142857146\n",
      "Epoch: 112 \n",
      "-----------\n",
      "Loss: 1.8389462232589722| Acc: 38.095238095238095\n",
      "Epoch: 113 \n",
      "-----------\n",
      "Loss: 1.8387219905853271| Acc: 37.142857142857146\n",
      "Epoch: 114 \n",
      "-----------\n",
      "Loss: 1.8391069173812866| Acc: 42.857142857142854\n",
      "Epoch: 115 \n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.8389359712600708| Acc: 41.904761904761905\n",
      "Epoch: 116 \n",
      "-----------\n",
      "Loss: 1.8387727737426758| Acc: 40.0\n",
      "Epoch: 117 \n",
      "-----------\n",
      "Loss: 1.8384846448898315| Acc: 39.04761904761905\n",
      "Epoch: 118 \n",
      "-----------\n",
      "Loss: 1.838424563407898| Acc: 39.04761904761905\n",
      "Epoch: 119 \n",
      "-----------\n",
      "Loss: 1.838716745376587| Acc: 38.095238095238095\n",
      "Epoch: 120 \n",
      "-----------\n",
      "Loss: 1.838187336921692| Acc: 38.095238095238095\n",
      "Epoch: 121 \n",
      "-----------\n",
      "Loss: 1.8386914730072021| Acc: 37.142857142857146\n",
      "Epoch: 122 \n",
      "-----------\n",
      "Loss: 1.8379749059677124| Acc: 38.095238095238095\n",
      "Epoch: 123 \n",
      "-----------\n",
      "Loss: 1.8386225700378418| Acc: 36.19047619047619\n",
      "Epoch: 124 \n",
      "-----------\n",
      "Loss: 1.837801456451416| Acc: 39.04761904761905\n",
      "Epoch: 125 \n",
      "-----------\n",
      "Loss: 1.8383196592330933| Acc: 36.19047619047619\n",
      "Epoch: 126 \n",
      "-----------\n",
      "Loss: 1.8376973867416382| Acc: 39.04761904761905\n",
      "Epoch: 127 \n",
      "-----------\n",
      "Loss: 1.8382422924041748| Acc: 36.19047619047619\n",
      "Epoch: 128 \n",
      "-----------\n",
      "Loss: 1.837608814239502| Acc: 39.04761904761905\n",
      "Epoch: 129 \n",
      "-----------\n",
      "Loss: 1.8383002281188965| Acc: 36.19047619047619\n",
      "Epoch: 130 \n",
      "-----------\n",
      "Loss: 1.8375515937805176| Acc: 38.095238095238095\n",
      "Epoch: 131 \n",
      "-----------\n",
      "Loss: 1.8374489545822144| Acc: 38.095238095238095\n",
      "Epoch: 132 \n",
      "-----------\n",
      "Loss: 1.870448112487793| Acc: 40.0\n",
      "Epoch: 133 \n",
      "-----------\n",
      "Loss: 1.8392713069915771| Acc: 39.04761904761905\n",
      "Epoch: 134 \n",
      "-----------\n",
      "Loss: 1.8407725095748901| Acc: 38.095238095238095\n",
      "Epoch: 135 \n",
      "-----------\n",
      "Loss: 1.839449405670166| Acc: 38.095238095238095\n",
      "Epoch: 136 \n",
      "-----------\n",
      "Loss: 1.8365545272827148| Acc: 37.142857142857146\n",
      "Epoch: 137 \n",
      "-----------\n",
      "Loss: 1.8368417024612427| Acc: 35.23809523809524\n",
      "Epoch: 138 \n",
      "-----------\n",
      "Loss: 1.837988018989563| Acc: 37.142857142857146\n",
      "Epoch: 139 \n",
      "-----------\n",
      "Loss: 1.8382517099380493| Acc: 38.095238095238095\n",
      "Epoch: 140 \n",
      "-----------\n",
      "Loss: 1.919601559638977| Acc: 35.23809523809524\n",
      "Epoch: 141 \n",
      "-----------\n",
      "Loss: 1.8997955322265625| Acc: 33.33333333333333\n",
      "Epoch: 142 \n",
      "-----------\n",
      "Loss: 1.8840018510818481| Acc: 41.904761904761905\n",
      "Epoch: 143 \n",
      "-----------\n",
      "Loss: 1.8358700275421143| Acc: 38.095238095238095\n",
      "Epoch: 144 \n",
      "-----------\n",
      "Loss: 1.859128475189209| Acc: 44.761904761904766\n",
      "Epoch: 145 \n",
      "-----------\n",
      "Loss: 1.9229373931884766| Acc: 37.142857142857146\n",
      "Epoch: 146 \n",
      "-----------\n",
      "Loss: 1.9923183917999268| Acc: 34.285714285714285\n",
      "Epoch: 147 \n",
      "-----------\n",
      "Loss: 1.9256757497787476| Acc: 34.285714285714285\n",
      "Epoch: 148 \n",
      "-----------\n",
      "Loss: 1.9068257808685303| Acc: 36.19047619047619\n",
      "Epoch: 149 \n",
      "-----------\n",
      "Loss: 1.8699308633804321| Acc: 37.142857142857146\n",
      "Epoch: 150 \n",
      "-----------\n",
      "Loss: 1.9075995683670044| Acc: 35.23809523809524\n",
      "Epoch: 151 \n",
      "-----------\n",
      "Loss: 1.8719972372055054| Acc: 40.95238095238095\n",
      "Epoch: 152 \n",
      "-----------\n",
      "Loss: 1.8722844123840332| Acc: 38.095238095238095\n",
      "Epoch: 153 \n",
      "-----------\n",
      "Loss: 1.9138906002044678| Acc: 39.04761904761905\n",
      "Epoch: 154 \n",
      "-----------\n",
      "Loss: 1.891215443611145| Acc: 40.0\n",
      "Epoch: 155 \n",
      "-----------\n",
      "Loss: 1.9152153730392456| Acc: 36.19047619047619\n",
      "Epoch: 156 \n",
      "-----------\n",
      "Loss: 1.8707910776138306| Acc: 38.095238095238095\n",
      "Epoch: 157 \n",
      "-----------\n",
      "Loss: 1.8625096082687378| Acc: 39.04761904761905\n",
      "Epoch: 158 \n",
      "-----------\n",
      "Loss: 1.8593097925186157| Acc: 39.04761904761905\n",
      "Epoch: 159 \n",
      "-----------\n",
      "Loss: 1.8682489395141602| Acc: 37.142857142857146\n",
      "Epoch: 160 \n",
      "-----------\n",
      "Loss: 1.8534122705459595| Acc: 44.761904761904766\n",
      "Epoch: 161 \n",
      "-----------\n",
      "Loss: 1.8568034172058105| Acc: 42.857142857142854\n",
      "Epoch: 162 \n",
      "-----------\n",
      "Loss: 1.8622643947601318| Acc: 41.904761904761905\n",
      "Epoch: 163 \n",
      "-----------\n",
      "Loss: 1.8699405193328857| Acc: 40.0\n",
      "Epoch: 164 \n",
      "-----------\n",
      "Loss: 1.8458504676818848| Acc: 41.904761904761905\n",
      "Epoch: 165 \n",
      "-----------\n",
      "Loss: 1.8404052257537842| Acc: 43.80952380952381\n",
      "Epoch: 166 \n",
      "-----------\n",
      "Loss: 1.8376480340957642| Acc: 44.761904761904766\n",
      "Epoch: 167 \n",
      "-----------\n",
      "Loss: 1.838315725326538| Acc: 41.904761904761905\n",
      "Epoch: 168 \n",
      "-----------\n",
      "Loss: 1.8375860452651978| Acc: 41.904761904761905\n",
      "Epoch: 169 \n",
      "-----------\n",
      "Loss: 1.8440319299697876| Acc: 38.095238095238095\n",
      "Epoch: 170 \n",
      "-----------\n",
      "Loss: 1.8370659351348877| Acc: 40.95238095238095\n",
      "Epoch: 171 \n",
      "-----------\n",
      "Loss: 1.8363432884216309| Acc: 40.0\n",
      "Epoch: 172 \n",
      "-----------\n",
      "Loss: 1.8360518217086792| Acc: 42.857142857142854\n",
      "Epoch: 173 \n",
      "-----------\n",
      "Loss: 1.8358169794082642| Acc: 40.0\n",
      "Epoch: 174 \n",
      "-----------\n",
      "Loss: 1.8666143417358398| Acc: 34.285714285714285\n",
      "Epoch: 175 \n",
      "-----------\n",
      "Loss: 1.8548935651779175| Acc: 37.142857142857146\n",
      "Epoch: 176 \n",
      "-----------\n",
      "Loss: 1.8673315048217773| Acc: 33.33333333333333\n",
      "Epoch: 177 \n",
      "-----------\n",
      "Loss: 1.8727906942367554| Acc: 32.38095238095238\n",
      "Epoch: 178 \n",
      "-----------\n",
      "Loss: 1.9157987833023071| Acc: 39.04761904761905\n",
      "Epoch: 179 \n",
      "-----------\n",
      "Loss: 1.8826327323913574| Acc: 40.0\n",
      "Epoch: 180 \n",
      "-----------\n",
      "Loss: 1.849265456199646| Acc: 42.857142857142854\n",
      "Epoch: 181 \n",
      "-----------\n",
      "Loss: 1.8450071811676025| Acc: 42.857142857142854\n",
      "Epoch: 182 \n",
      "-----------\n",
      "Loss: 1.8542288541793823| Acc: 40.95238095238095\n",
      "Epoch: 183 \n",
      "-----------\n",
      "Loss: 1.8416067361831665| Acc: 43.80952380952381\n",
      "Epoch: 184 \n",
      "-----------\n",
      "Loss: 1.92762291431427| Acc: 40.0\n",
      "Epoch: 185 \n",
      "-----------\n",
      "Loss: 1.8490664958953857| Acc: 40.0\n",
      "Epoch: 186 \n",
      "-----------\n",
      "Loss: 1.8450310230255127| Acc: 41.904761904761905\n",
      "Epoch: 187 \n",
      "-----------\n",
      "Loss: 1.8463858366012573| Acc: 41.904761904761905\n",
      "Epoch: 188 \n",
      "-----------\n",
      "Loss: 1.8454450368881226| Acc: 39.04761904761905\n",
      "Epoch: 189 \n",
      "-----------\n",
      "Loss: 1.8454523086547852| Acc: 39.04761904761905\n",
      "Epoch: 190 \n",
      "-----------\n",
      "Loss: 1.845476746559143| Acc: 39.04761904761905\n",
      "Epoch: 191 \n",
      "-----------\n",
      "Loss: 1.8458436727523804| Acc: 39.04761904761905\n",
      "Epoch: 192 \n",
      "-----------\n",
      "Loss: 1.841896891593933| Acc: 43.80952380952381\n",
      "Epoch: 193 \n",
      "-----------\n",
      "Loss: 1.841378927230835| Acc: 43.80952380952381\n",
      "Epoch: 194 \n",
      "-----------\n",
      "Loss: 1.8444591760635376| Acc: 40.95238095238095\n",
      "Epoch: 195 \n",
      "-----------\n",
      "Loss: 1.843962550163269| Acc: 41.904761904761905\n",
      "Epoch: 196 \n",
      "-----------\n",
      "Loss: 1.8431601524353027| Acc: 40.95238095238095\n",
      "Epoch: 197 \n",
      "-----------\n",
      "Loss: 1.8408787250518799| Acc: 42.857142857142854\n",
      "Epoch: 198 \n",
      "-----------\n",
      "Loss: 1.8399808406829834| Acc: 42.857142857142854\n",
      "Epoch: 199 \n",
      "-----------\n",
      "Loss: 1.840865969657898| Acc: 40.95238095238095\n",
      "Epoch: 200 \n",
      "-----------\n",
      "Loss: 1.8393642902374268| Acc: 41.904761904761905\n",
      "Epoch: 201 \n",
      "-----------\n",
      "Loss: 1.8402522802352905| Acc: 41.904761904761905\n",
      "Epoch: 202 \n",
      "-----------\n",
      "Loss: 1.8392046689987183| Acc: 40.95238095238095\n",
      "Epoch: 203 \n",
      "-----------\n",
      "Loss: 1.8762112855911255| Acc: 43.80952380952381\n",
      "Epoch: 204 \n",
      "-----------\n",
      "Loss: 1.8359454870224| Acc: 43.80952380952381\n",
      "Epoch: 205 \n",
      "-----------\n",
      "Loss: 1.854714035987854| Acc: 43.80952380952381\n",
      "Epoch: 206 \n",
      "-----------\n",
      "Loss: 1.8342393636703491| Acc: 43.80952380952381\n",
      "Epoch: 207 \n",
      "-----------\n",
      "Loss: 1.8342680931091309| Acc: 42.857142857142854\n",
      "Epoch: 208 \n",
      "-----------\n",
      "Loss: 1.8343532085418701| Acc: 41.904761904761905\n",
      "Epoch: 209 \n",
      "-----------\n",
      "Loss: 1.851296067237854| Acc: 39.04761904761905\n",
      "Epoch: 210 \n",
      "-----------\n",
      "Loss: 1.8411787748336792| Acc: 41.904761904761905\n",
      "Epoch: 211 \n",
      "-----------\n",
      "Loss: 1.8398293256759644| Acc: 42.857142857142854\n",
      "Epoch: 212 \n",
      "-----------\n",
      "Loss: 1.8524824380874634| Acc: 39.04761904761905\n",
      "Epoch: 213 \n",
      "-----------\n",
      "Loss: 1.84017014503479| Acc: 40.95238095238095\n",
      "Epoch: 214 \n",
      "-----------\n",
      "Loss: 1.8428711891174316| Acc: 40.0\n",
      "Epoch: 215 \n",
      "-----------\n",
      "Loss: 1.8538446426391602| Acc: 41.904761904761905\n",
      "Epoch: 216 \n",
      "-----------\n",
      "Loss: 1.842038869857788| Acc: 43.80952380952381\n",
      "Epoch: 217 \n",
      "-----------\n",
      "Loss: 1.8382264375686646| Acc: 40.95238095238095\n",
      "Epoch: 218 \n",
      "-----------\n",
      "Loss: 1.8376789093017578| Acc: 40.95238095238095\n",
      "Epoch: 219 \n",
      "-----------\n",
      "Loss: 1.8356084823608398| Acc: 43.80952380952381\n",
      "Epoch: 220 \n",
      "-----------\n",
      "Loss: 1.8356395959854126| Acc: 43.80952380952381\n",
      "Epoch: 221 \n",
      "-----------\n",
      "Loss: 1.8354092836380005| Acc: 43.80952380952381\n",
      "Epoch: 222 \n",
      "-----------\n",
      "Loss: 1.8351571559906006| Acc: 42.857142857142854\n",
      "Epoch: 223 \n",
      "-----------\n",
      "Loss: 1.8634403944015503| Acc: 43.80952380952381\n",
      "Epoch: 224 \n",
      "-----------\n",
      "Loss: 1.8584092855453491| Acc: 44.761904761904766\n",
      "Epoch: 225 \n",
      "-----------\n",
      "Loss: 1.888675332069397| Acc: 34.285714285714285\n",
      "Epoch: 226 \n",
      "-----------\n",
      "Loss: 1.8626532554626465| Acc: 40.95238095238095\n",
      "Epoch: 227 \n",
      "-----------\n",
      "Loss: 1.8512336015701294| Acc: 42.857142857142854\n",
      "Epoch: 228 \n",
      "-----------\n",
      "Loss: 1.8647578954696655| Acc: 41.904761904761905\n",
      "Epoch: 229 \n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.840505838394165| Acc: 45.714285714285715\n",
      "Epoch: 230 \n",
      "-----------\n",
      "Loss: 1.8768681287765503| Acc: 40.95238095238095\n",
      "Epoch: 231 \n",
      "-----------\n",
      "Loss: 1.8368860483169556| Acc: 44.761904761904766\n",
      "Epoch: 232 \n",
      "-----------\n",
      "Loss: 1.8556432723999023| Acc: 40.95238095238095\n",
      "Epoch: 233 \n",
      "-----------\n",
      "Loss: 1.8442920446395874| Acc: 43.80952380952381\n",
      "Epoch: 234 \n",
      "-----------\n",
      "Loss: 1.8574211597442627| Acc: 40.95238095238095\n",
      "Epoch: 235 \n",
      "-----------\n",
      "Loss: 1.8478026390075684| Acc: 40.95238095238095\n",
      "Epoch: 236 \n",
      "-----------\n",
      "Loss: 1.8461371660232544| Acc: 42.857142857142854\n",
      "Epoch: 237 \n",
      "-----------\n",
      "Loss: 1.8407279253005981| Acc: 41.904761904761905\n",
      "Epoch: 238 \n",
      "-----------\n",
      "Loss: 1.8507875204086304| Acc: 41.904761904761905\n",
      "Epoch: 239 \n",
      "-----------\n",
      "Loss: 1.8483511209487915| Acc: 41.904761904761905\n",
      "Epoch: 240 \n",
      "-----------\n",
      "Loss: 1.8368901014328003| Acc: 42.857142857142854\n",
      "Epoch: 241 \n",
      "-----------\n",
      "Loss: 1.835658311843872| Acc: 42.857142857142854\n",
      "Epoch: 242 \n",
      "-----------\n",
      "Loss: 1.835358738899231| Acc: 42.857142857142854\n",
      "Epoch: 243 \n",
      "-----------\n",
      "Loss: 1.8356894254684448| Acc: 41.904761904761905\n",
      "Epoch: 244 \n",
      "-----------\n",
      "Loss: 1.8357270956039429| Acc: 41.904761904761905\n",
      "Epoch: 245 \n",
      "-----------\n",
      "Loss: 1.835660457611084| Acc: 41.904761904761905\n",
      "Epoch: 246 \n",
      "-----------\n",
      "Loss: 1.8350759744644165| Acc: 41.904761904761905\n",
      "Epoch: 247 \n",
      "-----------\n",
      "Loss: 1.8350836038589478| Acc: 41.904761904761905\n",
      "Epoch: 248 \n",
      "-----------\n",
      "Loss: 1.8336975574493408| Acc: 41.904761904761905\n",
      "Epoch: 249 \n",
      "-----------\n",
      "Loss: 1.833095908164978| Acc: 43.80952380952381\n",
      "Epoch: 250 \n",
      "-----------\n",
      "Loss: 1.8354885578155518| Acc: 40.95238095238095\n",
      "Epoch: 251 \n",
      "-----------\n",
      "Loss: 1.8345088958740234| Acc: 41.904761904761905\n",
      "Epoch: 252 \n",
      "-----------\n",
      "Loss: 1.8413078784942627| Acc: 41.904761904761905\n",
      "Epoch: 253 \n",
      "-----------\n",
      "Loss: 1.8625761270523071| Acc: 39.04761904761905\n",
      "Epoch: 254 \n",
      "-----------\n",
      "Loss: 1.8536866903305054| Acc: 34.285714285714285\n",
      "Epoch: 255 \n",
      "-----------\n",
      "Loss: 1.8337509632110596| Acc: 44.761904761904766\n",
      "Epoch: 256 \n",
      "-----------\n",
      "Loss: 1.8362869024276733| Acc: 44.761904761904766\n",
      "Epoch: 257 \n",
      "-----------\n",
      "Loss: 1.8349263668060303| Acc: 42.857142857142854\n",
      "Epoch: 258 \n",
      "-----------\n",
      "Loss: 1.8349757194519043| Acc: 42.857142857142854\n",
      "Epoch: 259 \n",
      "-----------\n",
      "Loss: 1.835305094718933| Acc: 42.857142857142854\n",
      "Epoch: 260 \n",
      "-----------\n",
      "Loss: 1.8352171182632446| Acc: 42.857142857142854\n",
      "Epoch: 261 \n",
      "-----------\n",
      "Loss: 1.8349418640136719| Acc: 42.857142857142854\n",
      "Epoch: 262 \n",
      "-----------\n",
      "Loss: 1.8350565433502197| Acc: 42.857142857142854\n",
      "Epoch: 263 \n",
      "-----------\n",
      "Loss: 1.8348701000213623| Acc: 42.857142857142854\n",
      "Epoch: 264 \n",
      "-----------\n",
      "Loss: 1.8349061012268066| Acc: 42.857142857142854\n",
      "Epoch: 265 \n",
      "-----------\n",
      "Loss: 1.834350824356079| Acc: 43.80952380952381\n",
      "Epoch: 266 \n",
      "-----------\n",
      "Loss: 1.8341953754425049| Acc: 42.857142857142854\n",
      "Epoch: 267 \n",
      "-----------\n",
      "Loss: 1.8340638875961304| Acc: 42.857142857142854\n",
      "Epoch: 268 \n",
      "-----------\n",
      "Loss: 1.8342492580413818| Acc: 42.857142857142854\n",
      "Epoch: 269 \n",
      "-----------\n",
      "Loss: 1.8340319395065308| Acc: 42.857142857142854\n",
      "Epoch: 270 \n",
      "-----------\n",
      "Loss: 1.834083914756775| Acc: 42.857142857142854\n",
      "Epoch: 271 \n",
      "-----------\n",
      "Loss: 1.8334636688232422| Acc: 42.857142857142854\n",
      "Epoch: 272 \n",
      "-----------\n",
      "Loss: 1.8332805633544922| Acc: 42.857142857142854\n",
      "Epoch: 273 \n",
      "-----------\n",
      "Loss: 1.8327313661575317| Acc: 41.904761904761905\n",
      "Epoch: 274 \n",
      "-----------\n",
      "Loss: 1.832476258277893| Acc: 42.857142857142854\n",
      "Epoch: 275 \n",
      "-----------\n",
      "Loss: 1.8321468830108643| Acc: 42.857142857142854\n",
      "Epoch: 276 \n",
      "-----------\n",
      "Loss: 1.8326663970947266| Acc: 42.857142857142854\n",
      "Epoch: 277 \n",
      "-----------\n",
      "Loss: 1.8329874277114868| Acc: 44.761904761904766\n",
      "Epoch: 278 \n",
      "-----------\n",
      "Loss: 1.8458625078201294| Acc: 40.95238095238095\n",
      "Epoch: 279 \n",
      "-----------\n",
      "Loss: 1.8313039541244507| Acc: 44.761904761904766\n",
      "Epoch: 280 \n",
      "-----------\n",
      "Loss: 1.8284733295440674| Acc: 46.666666666666664\n",
      "Epoch: 281 \n",
      "-----------\n",
      "Loss: 1.8519182205200195| Acc: 42.857142857142854\n",
      "Epoch: 282 \n",
      "-----------\n",
      "Loss: 1.8848930597305298| Acc: 38.095238095238095\n",
      "Epoch: 283 \n",
      "-----------\n",
      "Loss: 1.834811806678772| Acc: 41.904761904761905\n",
      "Epoch: 284 \n",
      "-----------\n",
      "Loss: 1.835019826889038| Acc: 41.904761904761905\n",
      "Epoch: 285 \n",
      "-----------\n",
      "Loss: 1.8344157934188843| Acc: 41.904761904761905\n",
      "Epoch: 286 \n",
      "-----------\n",
      "Loss: 1.8693948984146118| Acc: 39.04761904761905\n",
      "Epoch: 287 \n",
      "-----------\n",
      "Loss: 1.8490078449249268| Acc: 40.95238095238095\n",
      "Epoch: 288 \n",
      "-----------\n",
      "Loss: 1.8491915464401245| Acc: 40.0\n",
      "Epoch: 289 \n",
      "-----------\n",
      "Loss: 1.8443042039871216| Acc: 40.95238095238095\n",
      "Epoch: 290 \n",
      "-----------\n",
      "Loss: 1.8307063579559326| Acc: 42.857142857142854\n",
      "Epoch: 291 \n",
      "-----------\n",
      "Loss: 1.8310024738311768| Acc: 42.857142857142854\n",
      "Epoch: 292 \n",
      "-----------\n",
      "Loss: 1.830221176147461| Acc: 43.80952380952381\n",
      "Epoch: 293 \n",
      "-----------\n",
      "Loss: 1.8306617736816406| Acc: 43.80952380952381\n",
      "Epoch: 294 \n",
      "-----------\n",
      "Loss: 1.8302276134490967| Acc: 42.857142857142854\n",
      "Epoch: 295 \n",
      "-----------\n",
      "Loss: 1.8304070234298706| Acc: 42.857142857142854\n",
      "Epoch: 296 \n",
      "-----------\n",
      "Loss: 1.8304779529571533| Acc: 42.857142857142854\n",
      "Epoch: 297 \n",
      "-----------\n",
      "Loss: 1.8300397396087646| Acc: 42.857142857142854\n",
      "Epoch: 298 \n",
      "-----------\n",
      "Loss: 1.8300223350524902| Acc: 42.857142857142854\n",
      "Epoch: 299 \n",
      "-----------\n",
      "Loss: 1.8299765586853027| Acc: 42.857142857142854\n",
      "Epoch: 300 \n",
      "-----------\n",
      "Loss: 1.8299418687820435| Acc: 42.857142857142854\n",
      "Epoch: 301 \n",
      "-----------\n",
      "Loss: 1.8296030759811401| Acc: 42.857142857142854\n",
      "Epoch: 302 \n",
      "-----------\n",
      "Loss: 1.8294848203659058| Acc: 42.857142857142854\n",
      "Epoch: 303 \n",
      "-----------\n",
      "Loss: 1.8296335935592651| Acc: 42.857142857142854\n",
      "Epoch: 304 \n",
      "-----------\n",
      "Loss: 1.8294360637664795| Acc: 42.857142857142854\n",
      "Epoch: 305 \n",
      "-----------\n",
      "Loss: 1.829361081123352| Acc: 42.857142857142854\n",
      "Epoch: 306 \n",
      "-----------\n",
      "Loss: 1.8292529582977295| Acc: 42.857142857142854\n",
      "Epoch: 307 \n",
      "-----------\n",
      "Loss: 1.8289421796798706| Acc: 43.80952380952381\n",
      "Epoch: 308 \n",
      "-----------\n",
      "Loss: 1.8290772438049316| Acc: 41.904761904761905\n",
      "Epoch: 309 \n",
      "-----------\n",
      "Loss: 1.829872488975525| Acc: 42.857142857142854\n",
      "Epoch: 310 \n",
      "-----------\n",
      "Loss: 1.8293083906173706| Acc: 41.904761904761905\n",
      "Epoch: 311 \n",
      "-----------\n",
      "Loss: 1.835477590560913| Acc: 41.904761904761905\n",
      "Epoch: 312 \n",
      "-----------\n",
      "Loss: 1.8303178548812866| Acc: 42.857142857142854\n",
      "Epoch: 313 \n",
      "-----------\n",
      "Loss: 1.830053687095642| Acc: 43.80952380952381\n",
      "Epoch: 314 \n",
      "-----------\n",
      "Loss: 1.8300819396972656| Acc: 42.857142857142854\n",
      "Epoch: 315 \n",
      "-----------\n",
      "Loss: 1.8298430442810059| Acc: 42.857142857142854\n",
      "Epoch: 316 \n",
      "-----------\n",
      "Loss: 1.8299459218978882| Acc: 42.857142857142854\n",
      "Epoch: 317 \n",
      "-----------\n",
      "Loss: 1.8291062116622925| Acc: 44.761904761904766\n",
      "Epoch: 318 \n",
      "-----------\n",
      "Loss: 1.829473853111267| Acc: 44.761904761904766\n",
      "Epoch: 319 \n",
      "-----------\n",
      "Loss: 1.8275867700576782| Acc: 42.857142857142854\n",
      "Epoch: 320 \n",
      "-----------\n",
      "Loss: 1.8337342739105225| Acc: 40.95238095238095\n",
      "Epoch: 321 \n",
      "-----------\n",
      "Loss: 1.8293203115463257| Acc: 40.0\n",
      "Epoch: 322 \n",
      "-----------\n",
      "Loss: 1.8329575061798096| Acc: 40.0\n",
      "Epoch: 323 \n",
      "-----------\n",
      "Loss: 1.8288275003433228| Acc: 43.80952380952381\n",
      "Epoch: 324 \n",
      "-----------\n",
      "Loss: 1.8279519081115723| Acc: 41.904761904761905\n",
      "Epoch: 325 \n",
      "-----------\n",
      "Loss: 1.827026128768921| Acc: 41.904761904761905\n",
      "Epoch: 326 \n",
      "-----------\n",
      "Loss: 1.8280764818191528| Acc: 43.80952380952381\n",
      "Epoch: 327 \n",
      "-----------\n",
      "Loss: 1.8267121315002441| Acc: 40.95238095238095\n",
      "Epoch: 328 \n",
      "-----------\n",
      "Loss: 1.826227068901062| Acc: 40.95238095238095\n",
      "Epoch: 329 \n",
      "-----------\n",
      "Loss: 1.8310601711273193| Acc: 40.0\n",
      "Epoch: 330 \n",
      "-----------\n",
      "Loss: 1.8665087223052979| Acc: 36.19047619047619\n",
      "Epoch: 331 \n",
      "-----------\n",
      "Loss: 1.8547040224075317| Acc: 46.666666666666664\n",
      "Epoch: 332 \n",
      "-----------\n",
      "Loss: 1.8271344900131226| Acc: 44.761904761904766\n",
      "Epoch: 333 \n",
      "-----------\n",
      "Loss: 1.8279496431350708| Acc: 42.857142857142854\n",
      "Epoch: 334 \n",
      "-----------\n",
      "Loss: 1.8286179304122925| Acc: 45.714285714285715\n",
      "Epoch: 335 \n",
      "-----------\n",
      "Loss: 1.82870352268219| Acc: 43.80952380952381\n",
      "Epoch: 336 \n",
      "-----------\n",
      "Loss: 1.8278164863586426| Acc: 43.80952380952381\n",
      "Epoch: 337 \n",
      "-----------\n",
      "Loss: 1.829590916633606| Acc: 43.80952380952381\n",
      "Epoch: 338 \n",
      "-----------\n",
      "Loss: 1.828037977218628| Acc: 44.761904761904766\n",
      "Epoch: 339 \n",
      "-----------\n",
      "Loss: 1.8298982381820679| Acc: 44.761904761904766\n",
      "Epoch: 340 \n",
      "-----------\n",
      "Loss: 1.8273930549621582| Acc: 43.80952380952381\n",
      "Epoch: 341 \n",
      "-----------\n",
      "Loss: 1.8816522359848022| Acc: 39.04761904761905\n",
      "Epoch: 342 \n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.842225193977356| Acc: 43.80952380952381\n",
      "Epoch: 343 \n",
      "-----------\n",
      "Loss: 1.828458547592163| Acc: 44.761904761904766\n",
      "Epoch: 344 \n",
      "-----------\n",
      "Loss: 1.8484910726547241| Acc: 42.857142857142854\n",
      "Epoch: 345 \n",
      "-----------\n",
      "Loss: 1.8789640665054321| Acc: 41.904761904761905\n",
      "Epoch: 346 \n",
      "-----------\n",
      "Loss: 1.8339757919311523| Acc: 43.80952380952381\n",
      "Epoch: 347 \n",
      "-----------\n",
      "Loss: 1.8249870538711548| Acc: 46.666666666666664\n",
      "Epoch: 348 \n",
      "-----------\n",
      "Loss: 1.8252041339874268| Acc: 45.714285714285715\n",
      "Epoch: 349 \n",
      "-----------\n",
      "Loss: 1.8251793384552002| Acc: 45.714285714285715\n",
      "Epoch: 350 \n",
      "-----------\n",
      "Loss: 1.825150966644287| Acc: 43.80952380952381\n",
      "Epoch: 351 \n",
      "-----------\n",
      "Loss: 1.824690341949463| Acc: 43.80952380952381\n",
      "Epoch: 352 \n",
      "-----------\n",
      "Loss: 1.8257402181625366| Acc: 42.857142857142854\n",
      "Epoch: 353 \n",
      "-----------\n",
      "Loss: 1.8255411386489868| Acc: 42.857142857142854\n",
      "Epoch: 354 \n",
      "-----------\n",
      "Loss: 1.8255146741867065| Acc: 42.857142857142854\n",
      "Epoch: 355 \n",
      "-----------\n",
      "Loss: 1.8245385885238647| Acc: 43.80952380952381\n",
      "Epoch: 356 \n",
      "-----------\n",
      "Loss: 1.8244935274124146| Acc: 44.761904761904766\n",
      "Epoch: 357 \n",
      "-----------\n",
      "Loss: 1.8243937492370605| Acc: 44.761904761904766\n",
      "Epoch: 358 \n",
      "-----------\n",
      "Loss: 1.824047327041626| Acc: 44.761904761904766\n",
      "Epoch: 359 \n",
      "-----------\n",
      "Loss: 1.8247301578521729| Acc: 44.761904761904766\n",
      "Epoch: 360 \n",
      "-----------\n",
      "Loss: 1.8237608671188354| Acc: 44.761904761904766\n",
      "Epoch: 361 \n",
      "-----------\n",
      "Loss: 1.824527382850647| Acc: 44.761904761904766\n",
      "Epoch: 362 \n",
      "-----------\n",
      "Loss: 1.8238942623138428| Acc: 44.761904761904766\n",
      "Epoch: 363 \n",
      "-----------\n",
      "Loss: 1.82391357421875| Acc: 44.761904761904766\n",
      "Epoch: 364 \n",
      "-----------\n",
      "Loss: 1.82427179813385| Acc: 43.80952380952381\n",
      "Epoch: 365 \n",
      "-----------\n",
      "Loss: 1.823660969734192| Acc: 44.761904761904766\n",
      "Epoch: 366 \n",
      "-----------\n",
      "Loss: 1.8232592344284058| Acc: 43.80952380952381\n",
      "Epoch: 367 \n",
      "-----------\n",
      "Loss: 1.823103427886963| Acc: 42.857142857142854\n",
      "Epoch: 368 \n",
      "-----------\n",
      "Loss: 1.82358717918396| Acc: 41.904761904761905\n",
      "Epoch: 369 \n",
      "-----------\n",
      "Loss: 1.8227750062942505| Acc: 42.857142857142854\n",
      "Epoch: 370 \n",
      "-----------\n",
      "Loss: 1.8232530355453491| Acc: 42.857142857142854\n",
      "Epoch: 371 \n",
      "-----------\n",
      "Loss: 1.8228363990783691| Acc: 42.857142857142854\n",
      "Epoch: 372 \n",
      "-----------\n",
      "Loss: 1.8226697444915771| Acc: 43.80952380952381\n",
      "Epoch: 373 \n",
      "-----------\n",
      "Loss: 1.8221192359924316| Acc: 45.714285714285715\n",
      "Epoch: 374 \n",
      "-----------\n",
      "Loss: 1.9264525175094604| Acc: 36.19047619047619\n",
      "Epoch: 375 \n",
      "-----------\n",
      "Loss: 1.8348468542099| Acc: 42.857142857142854\n",
      "Epoch: 376 \n",
      "-----------\n",
      "Loss: 1.824806571006775| Acc: 45.714285714285715\n",
      "Epoch: 377 \n",
      "-----------\n",
      "Loss: 1.8250175714492798| Acc: 45.714285714285715\n",
      "Epoch: 378 \n",
      "-----------\n",
      "Loss: 1.821279525756836| Acc: 44.761904761904766\n",
      "Epoch: 379 \n",
      "-----------\n",
      "Loss: 1.8221806287765503| Acc: 46.666666666666664\n",
      "Epoch: 380 \n",
      "-----------\n",
      "Loss: 1.8204398155212402| Acc: 46.666666666666664\n",
      "Epoch: 381 \n",
      "-----------\n",
      "Loss: 1.8214112520217896| Acc: 44.761904761904766\n",
      "Epoch: 382 \n",
      "-----------\n",
      "Loss: 1.8211292028427124| Acc: 44.761904761904766\n",
      "Epoch: 383 \n",
      "-----------\n",
      "Loss: 1.8213608264923096| Acc: 43.80952380952381\n",
      "Epoch: 384 \n",
      "-----------\n",
      "Loss: 1.8213380575180054| Acc: 42.857142857142854\n",
      "Epoch: 385 \n",
      "-----------\n",
      "Loss: 1.8212352991104126| Acc: 42.857142857142854\n",
      "Epoch: 386 \n",
      "-----------\n",
      "Loss: 1.8212934732437134| Acc: 42.857142857142854\n",
      "Epoch: 387 \n",
      "-----------\n",
      "Loss: 1.8207850456237793| Acc: 42.857142857142854\n",
      "Epoch: 388 \n",
      "-----------\n",
      "Loss: 1.821175456047058| Acc: 42.857142857142854\n",
      "Epoch: 389 \n",
      "-----------\n",
      "Loss: 1.8211150169372559| Acc: 42.857142857142854\n",
      "Epoch: 390 \n",
      "-----------\n",
      "Loss: 1.8204944133758545| Acc: 42.857142857142854\n",
      "Epoch: 391 \n",
      "-----------\n",
      "Loss: 1.8211326599121094| Acc: 43.80952380952381\n",
      "Epoch: 392 \n",
      "-----------\n",
      "Loss: 1.8208708763122559| Acc: 42.857142857142854\n",
      "Epoch: 393 \n",
      "-----------\n",
      "Loss: 1.820936918258667| Acc: 43.80952380952381\n",
      "Epoch: 394 \n",
      "-----------\n",
      "Loss: 1.8210829496383667| Acc: 42.857142857142854\n",
      "Epoch: 395 \n",
      "-----------\n",
      "Loss: 1.8209255933761597| Acc: 42.857142857142854\n",
      "Epoch: 396 \n",
      "-----------\n",
      "Loss: 1.8207370042800903| Acc: 42.857142857142854\n",
      "Epoch: 397 \n",
      "-----------\n",
      "Loss: 1.8203365802764893| Acc: 42.857142857142854\n",
      "Epoch: 398 \n",
      "-----------\n",
      "Loss: 1.820428729057312| Acc: 42.857142857142854\n",
      "Epoch: 399 \n",
      "-----------\n",
      "Loss: 1.8205522298812866| Acc: 44.761904761904766\n",
      "Epoch: 400 \n",
      "-----------\n",
      "Loss: 1.8206846714019775| Acc: 43.80952380952381\n",
      "Epoch: 401 \n",
      "-----------\n",
      "Loss: 1.820383906364441| Acc: 44.761904761904766\n",
      "Epoch: 402 \n",
      "-----------\n",
      "Loss: 1.8196113109588623| Acc: 43.80952380952381\n",
      "Epoch: 403 \n",
      "-----------\n",
      "Loss: 1.8202228546142578| Acc: 43.80952380952381\n",
      "Epoch: 404 \n",
      "-----------\n",
      "Loss: 1.819642424583435| Acc: 43.80952380952381\n",
      "Epoch: 405 \n",
      "-----------\n",
      "Loss: 1.8206672668457031| Acc: 44.761904761904766\n",
      "Epoch: 406 \n",
      "-----------\n",
      "Loss: 1.8205329179763794| Acc: 44.761904761904766\n",
      "Epoch: 407 \n",
      "-----------\n",
      "Loss: 1.8197332620620728| Acc: 44.761904761904766\n",
      "Epoch: 408 \n",
      "-----------\n",
      "Loss: 1.8203425407409668| Acc: 44.761904761904766\n",
      "Epoch: 409 \n",
      "-----------\n",
      "Loss: 1.819773554801941| Acc: 44.761904761904766\n",
      "Epoch: 410 \n",
      "-----------\n",
      "Loss: 1.8437323570251465| Acc: 38.095238095238095\n",
      "Epoch: 411 \n",
      "-----------\n",
      "Loss: 1.835970163345337| Acc: 42.857142857142854\n",
      "Epoch: 412 \n",
      "-----------\n",
      "Loss: 1.8357605934143066| Acc: 41.904761904761905\n",
      "Epoch: 413 \n",
      "-----------\n",
      "Loss: 1.824357271194458| Acc: 44.761904761904766\n",
      "Epoch: 414 \n",
      "-----------\n",
      "Loss: 1.82343327999115| Acc: 44.761904761904766\n",
      "Epoch: 415 \n",
      "-----------\n",
      "Loss: 1.8216296434402466| Acc: 45.714285714285715\n",
      "Epoch: 416 \n",
      "-----------\n",
      "Loss: 1.821152925491333| Acc: 45.714285714285715\n",
      "Epoch: 417 \n",
      "-----------\n",
      "Loss: 1.820794939994812| Acc: 44.761904761904766\n",
      "Epoch: 418 \n",
      "-----------\n",
      "Loss: 1.8199535608291626| Acc: 45.714285714285715\n",
      "Epoch: 419 \n",
      "-----------\n",
      "Loss: 1.8197218179702759| Acc: 45.714285714285715\n",
      "Epoch: 420 \n",
      "-----------\n",
      "Loss: 1.8199816942214966| Acc: 44.761904761904766\n",
      "Epoch: 421 \n",
      "-----------\n",
      "Loss: 1.820838451385498| Acc: 45.714285714285715\n",
      "Epoch: 422 \n",
      "-----------\n",
      "Loss: 1.8195381164550781| Acc: 44.761904761904766\n",
      "Epoch: 423 \n",
      "-----------\n",
      "Loss: 1.8197818994522095| Acc: 44.761904761904766\n",
      "Epoch: 424 \n",
      "-----------\n",
      "Loss: 1.8189127445220947| Acc: 44.761904761904766\n",
      "Epoch: 425 \n",
      "-----------\n",
      "Loss: 1.8191521167755127| Acc: 44.761904761904766\n",
      "Epoch: 426 \n",
      "-----------\n",
      "Loss: 1.8194061517715454| Acc: 44.761904761904766\n",
      "Epoch: 427 \n",
      "-----------\n",
      "Loss: 1.8185125589370728| Acc: 44.761904761904766\n",
      "Epoch: 428 \n",
      "-----------\n",
      "Loss: 1.8190747499465942| Acc: 44.761904761904766\n",
      "Epoch: 429 \n",
      "-----------\n",
      "Loss: 1.8189767599105835| Acc: 45.714285714285715\n",
      "Epoch: 430 \n",
      "-----------\n",
      "Loss: 1.8185805082321167| Acc: 44.761904761904766\n",
      "Epoch: 431 \n",
      "-----------\n",
      "Loss: 1.8182625770568848| Acc: 45.714285714285715\n",
      "Epoch: 432 \n",
      "-----------\n",
      "Loss: 1.8178797960281372| Acc: 46.666666666666664\n",
      "Epoch: 433 \n",
      "-----------\n",
      "Loss: 1.8180593252182007| Acc: 45.714285714285715\n",
      "Epoch: 434 \n",
      "-----------\n",
      "Loss: 1.818155288696289| Acc: 44.761904761904766\n",
      "Epoch: 435 \n",
      "-----------\n",
      "Loss: 1.8178155422210693| Acc: 45.714285714285715\n",
      "Epoch: 436 \n",
      "-----------\n",
      "Loss: 1.8179246187210083| Acc: 43.80952380952381\n",
      "Epoch: 437 \n",
      "-----------\n",
      "Loss: 1.8181437253952026| Acc: 43.80952380952381\n",
      "Epoch: 438 \n",
      "-----------\n",
      "Loss: 1.8179986476898193| Acc: 45.714285714285715\n",
      "Epoch: 439 \n",
      "-----------\n",
      "Loss: 1.818077564239502| Acc: 43.80952380952381\n",
      "Epoch: 440 \n",
      "-----------\n",
      "Loss: 1.830212116241455| Acc: 42.857142857142854\n",
      "Epoch: 441 \n",
      "-----------\n",
      "Loss: 1.8218252658843994| Acc: 46.666666666666664\n",
      "Epoch: 442 \n",
      "-----------\n",
      "Loss: 1.8218148946762085| Acc: 44.761904761904766\n",
      "Epoch: 443 \n",
      "-----------\n",
      "Loss: 1.8195682764053345| Acc: 43.80952380952381\n",
      "Epoch: 444 \n",
      "-----------\n",
      "Loss: 1.8181809186935425| Acc: 46.666666666666664\n",
      "Epoch: 445 \n",
      "-----------\n",
      "Loss: 1.8179856538772583| Acc: 46.666666666666664\n",
      "Epoch: 446 \n",
      "-----------\n",
      "Loss: 1.8178114891052246| Acc: 46.666666666666664\n",
      "Epoch: 447 \n",
      "-----------\n",
      "Loss: 1.818194031715393| Acc: 45.714285714285715\n",
      "Epoch: 448 \n",
      "-----------\n",
      "Loss: 1.8175584077835083| Acc: 46.666666666666664\n",
      "Epoch: 449 \n",
      "-----------\n",
      "Loss: 1.8184762001037598| Acc: 43.80952380952381\n",
      "Epoch: 450 \n",
      "-----------\n",
      "Loss: 1.8191235065460205| Acc: 43.80952380952381\n",
      "Epoch: 451 \n",
      "-----------\n",
      "Loss: 1.8183283805847168| Acc: 43.80952380952381\n",
      "Epoch: 452 \n",
      "-----------\n",
      "Loss: 1.8184175491333008| Acc: 43.80952380952381\n",
      "Epoch: 453 \n",
      "-----------\n",
      "Loss: 1.8287498950958252| Acc: 43.80952380952381\n",
      "Epoch: 454 \n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.819365382194519| Acc: 42.857142857142854\n",
      "Epoch: 455 \n",
      "-----------\n",
      "Loss: 1.8192238807678223| Acc: 44.761904761904766\n",
      "Epoch: 456 \n",
      "-----------\n",
      "Loss: 1.8184959888458252| Acc: 43.80952380952381\n",
      "Epoch: 457 \n",
      "-----------\n",
      "Loss: 1.8181788921356201| Acc: 43.80952380952381\n",
      "Epoch: 458 \n",
      "-----------\n",
      "Loss: 1.8174917697906494| Acc: 43.80952380952381\n",
      "Epoch: 459 \n",
      "-----------\n",
      "Loss: 1.817496418952942| Acc: 43.80952380952381\n",
      "Epoch: 460 \n",
      "-----------\n",
      "Loss: 1.8169275522232056| Acc: 43.80952380952381\n",
      "Epoch: 461 \n",
      "-----------\n",
      "Loss: 1.8167731761932373| Acc: 43.80952380952381\n",
      "Epoch: 462 \n",
      "-----------\n",
      "Loss: 1.8165866136550903| Acc: 43.80952380952381\n",
      "Epoch: 463 \n",
      "-----------\n",
      "Loss: 1.817285418510437| Acc: 43.80952380952381\n",
      "Epoch: 464 \n",
      "-----------\n",
      "Loss: 1.8169163465499878| Acc: 43.80952380952381\n",
      "Epoch: 465 \n",
      "-----------\n",
      "Loss: 1.8169183731079102| Acc: 42.857142857142854\n",
      "Epoch: 466 \n",
      "-----------\n",
      "Loss: 1.8161466121673584| Acc: 45.714285714285715\n",
      "Epoch: 467 \n",
      "-----------\n",
      "Loss: 1.8169026374816895| Acc: 42.857142857142854\n",
      "Epoch: 468 \n",
      "-----------\n",
      "Loss: 1.8164951801300049| Acc: 43.80952380952381\n",
      "Epoch: 469 \n",
      "-----------\n",
      "Loss: 1.8165620565414429| Acc: 44.761904761904766\n",
      "Epoch: 470 \n",
      "-----------\n",
      "Loss: 1.8161786794662476| Acc: 43.80952380952381\n",
      "Epoch: 471 \n",
      "-----------\n",
      "Loss: 1.8160172700881958| Acc: 43.80952380952381\n",
      "Epoch: 472 \n",
      "-----------\n",
      "Loss: 1.8162508010864258| Acc: 43.80952380952381\n",
      "Epoch: 473 \n",
      "-----------\n",
      "Loss: 1.8160825967788696| Acc: 43.80952380952381\n",
      "Epoch: 474 \n",
      "-----------\n",
      "Loss: 1.8159945011138916| Acc: 43.80952380952381\n",
      "Epoch: 475 \n",
      "-----------\n",
      "Loss: 1.8156837224960327| Acc: 44.761904761904766\n",
      "Epoch: 476 \n",
      "-----------\n",
      "Loss: 1.8158067464828491| Acc: 43.80952380952381\n",
      "Epoch: 477 \n",
      "-----------\n",
      "Loss: 1.82596755027771| Acc: 42.857142857142854\n",
      "Epoch: 478 \n",
      "-----------\n",
      "Loss: 1.816472053527832| Acc: 45.714285714285715\n",
      "Epoch: 479 \n",
      "-----------\n",
      "Loss: 1.8162733316421509| Acc: 44.761904761904766\n",
      "Epoch: 480 \n",
      "-----------\n",
      "Loss: 1.8151946067810059| Acc: 45.714285714285715\n",
      "Epoch: 481 \n",
      "-----------\n",
      "Loss: 1.8184767961502075| Acc: 43.80952380952381\n",
      "Epoch: 482 \n",
      "-----------\n",
      "Loss: 1.815953016281128| Acc: 43.80952380952381\n",
      "Epoch: 483 \n",
      "-----------\n",
      "Loss: 1.8266048431396484| Acc: 40.95238095238095\n",
      "Epoch: 484 \n",
      "-----------\n",
      "Loss: 1.8151922225952148| Acc: 47.61904761904761\n",
      "Epoch: 485 \n",
      "-----------\n",
      "Loss: 1.8144690990447998| Acc: 46.666666666666664\n",
      "Epoch: 486 \n",
      "-----------\n",
      "Loss: 1.8954753875732422| Acc: 40.0\n",
      "Epoch: 487 \n",
      "-----------\n",
      "Loss: 1.8950062990188599| Acc: 39.04761904761905\n",
      "Epoch: 488 \n",
      "-----------\n",
      "Loss: 1.8755319118499756| Acc: 43.80952380952381\n",
      "Epoch: 489 \n",
      "-----------\n",
      "Loss: 1.8692485094070435| Acc: 42.857142857142854\n",
      "Epoch: 490 \n",
      "-----------\n",
      "Loss: 1.8548415899276733| Acc: 45.714285714285715\n",
      "Epoch: 491 \n",
      "-----------\n",
      "Loss: 1.8527699708938599| Acc: 43.80952380952381\n",
      "Epoch: 492 \n",
      "-----------\n",
      "Loss: 1.8247979879379272| Acc: 47.61904761904761\n",
      "Epoch: 493 \n",
      "-----------\n",
      "Loss: 1.824975609779358| Acc: 44.761904761904766\n",
      "Epoch: 494 \n",
      "-----------\n",
      "Loss: 1.8243762254714966| Acc: 47.61904761904761\n",
      "Epoch: 495 \n",
      "-----------\n",
      "Loss: 1.8470815420150757| Acc: 43.80952380952381\n",
      "Epoch: 496 \n",
      "-----------\n",
      "Loss: 1.9802250862121582| Acc: 33.33333333333333\n",
      "Epoch: 497 \n",
      "-----------\n",
      "Loss: 1.838150143623352| Acc: 42.857142857142854\n",
      "Epoch: 498 \n",
      "-----------\n",
      "Loss: 1.8205647468566895| Acc: 45.714285714285715\n",
      "Epoch: 499 \n",
      "-----------\n",
      "Loss: 1.8978185653686523| Acc: 35.23809523809524\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "epochs = 500\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch} \\n-----------\")\n",
    "    ### Training\n",
    "    train_step(model=loaded_model_0,\n",
    "                   data_loader=train_data_loader,\n",
    "                   loss_fn=loss_fn,\n",
    "                   optimizer=optimizer,\n",
    "                   accuracy_fn=accuracy_fn,\n",
    "                   device=device)\n",
    "    \n",
    "      \n",
    "\n",
    "#       ###testing\n",
    "#       test_step(model=model_0,\n",
    "#                    data_loader=test_data_loader,\n",
    "#                    loss_fn=loss_fn,\n",
    "#                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0791d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.0.weight',\n",
       "              tensor([[[[ 1.2736e-01, -2.4879e-01, -8.6805e-02],\n",
       "                        [ 9.9421e-02,  1.7928e-01, -2.1005e-01],\n",
       "                        [-8.3295e-02, -3.7298e-02, -2.2634e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9501e-02, -2.0520e-01,  1.1310e-04],\n",
       "                        [-3.1207e-01,  1.9089e-01, -2.3956e-01],\n",
       "                        [ 2.8661e-01,  2.8142e-01, -8.9007e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5945e-01, -2.2633e-01,  1.6707e-01],\n",
       "                        [ 1.8872e-01, -1.3070e-01, -2.1801e-01],\n",
       "                        [-2.8751e-01,  3.2580e-01, -1.4293e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6666e-01, -2.6929e-01, -6.4499e-02],\n",
       "                        [ 3.0797e-01,  2.6599e-01, -2.3795e-01],\n",
       "                        [ 2.4650e-01,  5.0345e-02, -2.2171e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0181e-01, -1.0298e-01, -2.4623e-02],\n",
       "                        [-1.3063e-01,  8.6783e-02, -3.2724e-02],\n",
       "                        [ 6.3938e-02,  5.4473e-02, -3.2724e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.8224e-01,  1.3260e-02, -2.8636e-01],\n",
       "                        [ 1.9402e-01, -3.1981e-01,  8.3905e-02],\n",
       "                        [-2.4199e-01, -1.9225e-01, -2.5192e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1061e-01, -4.4894e-02, -2.2630e-01],\n",
       "                        [ 1.9807e-01,  3.1695e-01,  2.3777e-01],\n",
       "                        [ 3.1607e-01, -1.9449e-01, -7.7969e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.4531e-02, -1.9411e-02, -1.2816e-01],\n",
       "                        [ 2.1132e-01, -1.9558e-01, -3.0427e-01],\n",
       "                        [ 3.5605e-01,  3.3337e-01, -1.4947e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9892e-01, -1.1984e-04,  3.0427e-01],\n",
       "                        [ 2.4512e-01, -2.7852e-01,  7.2158e-02],\n",
       "                        [-1.2526e-01,  2.4807e-01,  1.6369e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9098e-01,  1.7104e-01, -2.7741e-01],\n",
       "                        [ 2.5783e-01, -1.7084e-01,  2.9176e-01],\n",
       "                        [ 3.1480e-02,  2.4776e-01,  3.2277e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0459e-01, -2.2333e-02,  3.0814e-01],\n",
       "                        [ 7.1901e-02,  2.7568e-01, -5.0842e-02],\n",
       "                        [-1.6450e-01, -7.6546e-02, -1.1714e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0744e-01,  1.2581e-01,  1.1908e-01],\n",
       "                        [-9.1511e-02,  2.0156e-02,  2.1259e-01],\n",
       "                        [-3.0093e-01, -8.2154e-03,  2.0227e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4449e-01,  1.5311e-01,  2.2529e-01],\n",
       "                        [-2.4502e-01, -1.1109e-01,  8.6455e-02],\n",
       "                        [ 1.1102e-01, -1.8584e-01,  1.8682e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0641e-01,  1.5156e-01, -2.2145e-01],\n",
       "                        [ 2.6338e-01,  3.4571e-01,  1.3164e-01],\n",
       "                        [-9.2715e-03,  6.9271e-02,  1.2300e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7481e-01,  5.8070e-02,  1.2726e-01],\n",
       "                        [ 2.8321e-01, -2.7130e-01,  2.8638e-01],\n",
       "                        [ 9.0627e-02,  2.3482e-01,  2.1561e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2761e-01, -8.0862e-03, -1.8980e-01],\n",
       "                        [-2.6452e-01,  1.4537e-01,  2.0085e-01],\n",
       "                        [ 2.0721e-01,  1.3585e-01,  8.2098e-02]]]], device='cuda:0')),\n",
       "             ('conv1.0.bias',\n",
       "              tensor([ 0.0574, -0.2196,  0.0714,  0.1015,  0.0791,  0.2764,  0.1369,  0.0724,\n",
       "                      -0.2282,  0.0705, -0.0766, -0.2987, -0.1584, -0.2573, -0.1517, -0.2083],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.0.weight',\n",
       "              tensor([[[[-0.0242, -0.0481, -0.0182],\n",
       "                        [-0.0129,  0.0698,  0.0513],\n",
       "                        [ 0.0242, -0.0569, -0.0163]],\n",
       "              \n",
       "                       [[ 0.0279, -0.0260,  0.0528],\n",
       "                        [ 0.0340, -0.0558,  0.0677],\n",
       "                        [-0.0197, -0.0080, -0.0267]],\n",
       "              \n",
       "                       [[ 0.0035,  0.0694, -0.0432],\n",
       "                        [ 0.0078,  0.0462, -0.0395],\n",
       "                        [-0.0070,  0.0707,  0.0301]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0133, -0.0588,  0.0522],\n",
       "                        [ 0.0247,  0.0622,  0.0042],\n",
       "                        [-0.0390,  0.0429,  0.0505]],\n",
       "              \n",
       "                       [[ 0.0010, -0.0102,  0.0547],\n",
       "                        [ 0.0724,  0.0180,  0.0716],\n",
       "                        [-0.0051, -0.0047,  0.0801]],\n",
       "              \n",
       "                       [[-0.0197, -0.0298,  0.0060],\n",
       "                        [ 0.0143,  0.0496,  0.0305],\n",
       "                        [ 0.0171,  0.0641,  0.0104]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0206, -0.0709, -0.0685],\n",
       "                        [-0.0549,  0.0018, -0.0681],\n",
       "                        [-0.0385,  0.0497, -0.0719]],\n",
       "              \n",
       "                       [[ 0.0763,  0.0520, -0.0230],\n",
       "                        [ 0.0160, -0.0470,  0.0590],\n",
       "                        [-0.0469, -0.0730, -0.0184]],\n",
       "              \n",
       "                       [[ 0.0251,  0.0675, -0.0558],\n",
       "                        [-0.0214,  0.0787,  0.0833],\n",
       "                        [ 0.0333, -0.0506, -0.0405]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0453,  0.0811,  0.0012],\n",
       "                        [ 0.0300, -0.0082, -0.0517],\n",
       "                        [-0.0229, -0.0802, -0.0174]],\n",
       "              \n",
       "                       [[ 0.0340, -0.0207, -0.0176],\n",
       "                        [-0.0289,  0.0518,  0.0599],\n",
       "                        [ 0.0117, -0.0096, -0.0454]],\n",
       "              \n",
       "                       [[ 0.0017,  0.0552,  0.0708],\n",
       "                        [ 0.0449, -0.0594, -0.0148],\n",
       "                        [ 0.0033, -0.0042, -0.0629]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0459,  0.0531, -0.0595],\n",
       "                        [ 0.0383,  0.0741, -0.0750],\n",
       "                        [-0.0747,  0.0174, -0.0243]],\n",
       "              \n",
       "                       [[-0.0498, -0.0209, -0.0791],\n",
       "                        [-0.0089, -0.0171, -0.0454],\n",
       "                        [-0.0416, -0.0381, -0.0072]],\n",
       "              \n",
       "                       [[ 0.0313, -0.0338, -0.0332],\n",
       "                        [-0.0246, -0.0578, -0.0210],\n",
       "                        [ 0.0232, -0.0501,  0.0816]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0245, -0.0471,  0.0703],\n",
       "                        [ 0.0211, -0.0450,  0.0040],\n",
       "                        [ 0.0269,  0.0184, -0.0440]],\n",
       "              \n",
       "                       [[-0.0357,  0.0038, -0.0078],\n",
       "                        [-0.0749,  0.0179, -0.0054],\n",
       "                        [ 0.0183, -0.0283, -0.0437]],\n",
       "              \n",
       "                       [[-0.0121,  0.0566, -0.0676],\n",
       "                        [ 0.0439,  0.0625, -0.0362],\n",
       "                        [ 0.0737,  0.0327, -0.0670]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0332,  0.0024,  0.0301],\n",
       "                        [-0.0337, -0.0303,  0.0581],\n",
       "                        [-0.0468, -0.0298,  0.0188]],\n",
       "              \n",
       "                       [[ 0.0261,  0.0485, -0.0653],\n",
       "                        [-0.0553, -0.0459,  0.0235],\n",
       "                        [ 0.0247,  0.0377,  0.0059]],\n",
       "              \n",
       "                       [[-0.0260, -0.0413,  0.0674],\n",
       "                        [ 0.0521,  0.0615, -0.0510],\n",
       "                        [ 0.0231,  0.0654,  0.0821]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0527,  0.0387, -0.0403],\n",
       "                        [ 0.0377, -0.0287,  0.0304],\n",
       "                        [ 0.0344,  0.0814, -0.0260]],\n",
       "              \n",
       "                       [[-0.0235, -0.0172,  0.0342],\n",
       "                        [-0.0703, -0.0657,  0.0616],\n",
       "                        [ 0.0211, -0.0298,  0.0185]],\n",
       "              \n",
       "                       [[-0.0472,  0.0793, -0.0355],\n",
       "                        [ 0.0344, -0.0729,  0.0318],\n",
       "                        [ 0.0681, -0.0078,  0.0484]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0189, -0.0615, -0.0703],\n",
       "                        [-0.0766,  0.0057,  0.0290],\n",
       "                        [ 0.0777, -0.0099, -0.0389]],\n",
       "              \n",
       "                       [[-0.0561,  0.0474, -0.0277],\n",
       "                        [ 0.0517, -0.0107, -0.0616],\n",
       "                        [-0.0690,  0.0627, -0.0069]],\n",
       "              \n",
       "                       [[-0.0615,  0.0230,  0.0008],\n",
       "                        [-0.0812,  0.0637, -0.0357],\n",
       "                        [ 0.0833,  0.0535,  0.0410]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0301,  0.0541,  0.0121],\n",
       "                        [-0.0356,  0.0816,  0.0283],\n",
       "                        [ 0.0123, -0.0033,  0.0299]],\n",
       "              \n",
       "                       [[-0.0779, -0.0576, -0.0688],\n",
       "                        [ 0.0389,  0.0108,  0.0549],\n",
       "                        [ 0.0393,  0.0047,  0.0787]],\n",
       "              \n",
       "                       [[ 0.0831,  0.0184,  0.0434],\n",
       "                        [ 0.0085,  0.0088,  0.0277],\n",
       "                        [-0.0675,  0.0331, -0.0747]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0197,  0.0736,  0.0792],\n",
       "                        [ 0.0140,  0.0053,  0.0404],\n",
       "                        [ 0.0573,  0.0214,  0.0310]],\n",
       "              \n",
       "                       [[-0.0677,  0.0532, -0.0655],\n",
       "                        [ 0.0498, -0.0180,  0.0399],\n",
       "                        [ 0.0662, -0.0071, -0.0176]],\n",
       "              \n",
       "                       [[ 0.0867, -0.0144,  0.0547],\n",
       "                        [ 0.0403,  0.0327,  0.0507],\n",
       "                        [-0.0458, -0.0333, -0.0559]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0731, -0.0332, -0.0173],\n",
       "                        [ 0.0217, -0.0009, -0.0788],\n",
       "                        [ 0.0697, -0.0633,  0.0242]],\n",
       "              \n",
       "                       [[ 0.0509, -0.0512,  0.0781],\n",
       "                        [ 0.0403,  0.0734, -0.0598],\n",
       "                        [-0.0313,  0.0670,  0.0343]],\n",
       "              \n",
       "                       [[-0.0719, -0.0294,  0.0253],\n",
       "                        [ 0.0783, -0.0426, -0.0129],\n",
       "                        [-0.0656,  0.0661,  0.0636]]]], device='cuda:0')),\n",
       "             ('conv2.0.bias',\n",
       "              tensor([ 0.0675,  0.0692, -0.0015, -0.0592,  0.0066, -0.0782, -0.0298,  0.0306,\n",
       "                       0.0596, -0.0481,  0.0591,  0.0316, -0.0265, -0.0526, -0.0029,  0.0623,\n",
       "                       0.0807,  0.0017,  0.0555,  0.0546, -0.0562, -0.0060,  0.0245, -0.0299,\n",
       "                       0.0140,  0.0139,  0.0183,  0.0741,  0.0250, -0.0029,  0.0801, -0.0323],\n",
       "                     device='cuda:0')),\n",
       "             ('conv3.0.weight',\n",
       "              tensor([[[[ 0.0408,  0.0566,  0.0358],\n",
       "                        [ 0.0611,  0.0525, -0.0065],\n",
       "                        [-0.0091, -0.0080,  0.0083]],\n",
       "              \n",
       "                       [[-0.0458,  0.0266,  0.0155],\n",
       "                        [ 0.0329,  0.0016,  0.0266],\n",
       "                        [ 0.0367, -0.0058, -0.0521]],\n",
       "              \n",
       "                       [[-0.0210, -0.0330,  0.0567],\n",
       "                        [-0.0453, -0.0035,  0.0260],\n",
       "                        [ 0.0219,  0.0581, -0.0095]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0328,  0.0610,  0.0361],\n",
       "                        [ 0.0507,  0.0467, -0.0234],\n",
       "                        [-0.0225,  0.0318,  0.0555]],\n",
       "              \n",
       "                       [[ 0.0189,  0.0519, -0.0063],\n",
       "                        [ 0.0560, -0.0309, -0.0092],\n",
       "                        [ 0.0116, -0.0352, -0.0559]],\n",
       "              \n",
       "                       [[-0.0461,  0.0246, -0.0034],\n",
       "                        [ 0.0164, -0.0533,  0.0235],\n",
       "                        [-0.0312,  0.0329, -0.0024]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0309,  0.0146, -0.0117],\n",
       "                        [ 0.0068,  0.0440,  0.0221],\n",
       "                        [ 0.0057, -0.0336, -0.0185]],\n",
       "              \n",
       "                       [[ 0.0577, -0.0348,  0.0056],\n",
       "                        [-0.0461,  0.0142,  0.0333],\n",
       "                        [-0.0053, -0.0105,  0.0523]],\n",
       "              \n",
       "                       [[-0.0443, -0.0180,  0.0514],\n",
       "                        [-0.0454,  0.0171,  0.0032],\n",
       "                        [ 0.0320, -0.0395,  0.0171]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0350, -0.0081, -0.0325],\n",
       "                        [ 0.0550,  0.0560, -0.0285],\n",
       "                        [-0.0521,  0.0239,  0.0296]],\n",
       "              \n",
       "                       [[-0.0112, -0.0364,  0.0555],\n",
       "                        [-0.0404,  0.0474, -0.0034],\n",
       "                        [ 0.0051, -0.0375,  0.0461]],\n",
       "              \n",
       "                       [[-0.0461, -0.0219,  0.0485],\n",
       "                        [-0.0461, -0.0563,  0.0166],\n",
       "                        [-0.0052, -0.0097, -0.0338]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0388, -0.0273,  0.0007],\n",
       "                        [ 0.0354,  0.0236,  0.0359],\n",
       "                        [ 0.0555, -0.0071,  0.0459]],\n",
       "              \n",
       "                       [[ 0.0112,  0.0303, -0.0279],\n",
       "                        [ 0.0368, -0.0436, -0.0412],\n",
       "                        [-0.0247, -0.0473, -0.0551]],\n",
       "              \n",
       "                       [[ 0.0583, -0.0188, -0.0283],\n",
       "                        [-0.0141, -0.0511,  0.0186],\n",
       "                        [-0.0546,  0.0221, -0.0570]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0552,  0.0030, -0.0167],\n",
       "                        [-0.0332,  0.0318, -0.0266],\n",
       "                        [ 0.0559,  0.0458, -0.0293]],\n",
       "              \n",
       "                       [[-0.0553, -0.0564,  0.0495],\n",
       "                        [ 0.0559, -0.0560,  0.0240],\n",
       "                        [ 0.0319,  0.0071,  0.0149]],\n",
       "              \n",
       "                       [[-0.0318,  0.0027, -0.0169],\n",
       "                        [-0.0048,  0.0341,  0.0607],\n",
       "                        [-0.0327, -0.0099,  0.0147]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0291, -0.0395, -0.0218],\n",
       "                        [ 0.0203, -0.0487, -0.0591],\n",
       "                        [ 0.0256, -0.0325,  0.0070]],\n",
       "              \n",
       "                       [[ 0.0137, -0.0491,  0.0005],\n",
       "                        [-0.0543, -0.0227, -0.0526],\n",
       "                        [-0.0153,  0.0474, -0.0294]],\n",
       "              \n",
       "                       [[ 0.0414, -0.0488, -0.0182],\n",
       "                        [-0.0223, -0.0405, -0.0582],\n",
       "                        [ 0.0479,  0.0500, -0.0295]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0499,  0.0304, -0.0501],\n",
       "                        [ 0.0424, -0.0563,  0.0258],\n",
       "                        [ 0.0351, -0.0106,  0.0542]],\n",
       "              \n",
       "                       [[-0.0440,  0.0538, -0.0203],\n",
       "                        [ 0.0159, -0.0268, -0.0072],\n",
       "                        [-0.0179, -0.0082,  0.0489]],\n",
       "              \n",
       "                       [[-0.0194, -0.0050,  0.0210],\n",
       "                        [-0.0373, -0.0386, -0.0579],\n",
       "                        [ 0.0387, -0.0236,  0.0518]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0398, -0.0479,  0.0408],\n",
       "                        [ 0.0222, -0.0128, -0.0379],\n",
       "                        [-0.0213, -0.0474, -0.0539]],\n",
       "              \n",
       "                       [[ 0.0517,  0.0312, -0.0518],\n",
       "                        [-0.0562,  0.0123, -0.0320],\n",
       "                        [-0.0450, -0.0573, -0.0313]],\n",
       "              \n",
       "                       [[ 0.0011, -0.0446,  0.0302],\n",
       "                        [ 0.0399, -0.0113,  0.0250],\n",
       "                        [-0.0580, -0.0564,  0.0386]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0267, -0.0457,  0.0582],\n",
       "                        [-0.0266,  0.0269,  0.0250],\n",
       "                        [-0.0588, -0.0352,  0.0038]],\n",
       "              \n",
       "                       [[-0.0498,  0.0444, -0.0191],\n",
       "                        [ 0.0256, -0.0330, -0.0515],\n",
       "                        [ 0.0479, -0.0562,  0.0463]],\n",
       "              \n",
       "                       [[ 0.0550, -0.0277, -0.0079],\n",
       "                        [-0.0058,  0.0109, -0.0567],\n",
       "                        [-0.0231, -0.0543, -0.0187]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0273,  0.0078,  0.0047],\n",
       "                        [-0.0242,  0.0390,  0.0508],\n",
       "                        [-0.0330,  0.0466, -0.0417]],\n",
       "              \n",
       "                       [[ 0.0269, -0.0205, -0.0299],\n",
       "                        [-0.0269, -0.0237,  0.0242],\n",
       "                        [ 0.0527, -0.0242, -0.0065]],\n",
       "              \n",
       "                       [[ 0.0140,  0.0190,  0.0226],\n",
       "                        [-0.0009,  0.0598,  0.0574],\n",
       "                        [ 0.0443,  0.0021,  0.0569]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0511,  0.0422,  0.0276],\n",
       "                        [-0.0183,  0.0253,  0.0505],\n",
       "                        [ 0.0155, -0.0042, -0.0025]],\n",
       "              \n",
       "                       [[-0.0172,  0.0606, -0.0553],\n",
       "                        [ 0.0058,  0.0407, -0.0356],\n",
       "                        [-0.0127,  0.0338,  0.0483]],\n",
       "              \n",
       "                       [[-0.0229, -0.0067,  0.0432],\n",
       "                        [ 0.0126, -0.0232,  0.0038],\n",
       "                        [-0.0545, -0.0056,  0.0171]]]], device='cuda:0')),\n",
       "             ('conv3.0.bias',\n",
       "              tensor([ 3.8805e-04,  3.4145e-02,  2.6614e-02,  5.6009e-03, -9.3530e-03,\n",
       "                       1.4587e-02, -2.8905e-05,  4.6724e-02, -3.8147e-03,  5.2391e-02,\n",
       "                      -5.9718e-03, -4.2583e-02, -1.3778e-03, -2.4058e-02,  1.1120e-02,\n",
       "                       4.3190e-04,  4.1942e-02,  2.9596e-02,  5.4311e-02,  1.3638e-02,\n",
       "                       3.4214e-02, -2.6696e-02, -2.3060e-02,  5.6148e-02, -2.4261e-02,\n",
       "                       5.2113e-02, -3.8998e-02,  2.7430e-02,  5.4142e-02,  4.3926e-03,\n",
       "                      -4.1873e-02,  4.5463e-02, -1.0966e-02,  3.1286e-02,  3.5274e-02,\n",
       "                      -1.6855e-03, -2.1591e-02,  7.9667e-03,  2.7418e-02,  2.7068e-02,\n",
       "                      -3.2656e-02,  4.0465e-03, -1.6281e-02, -4.9382e-02,  2.9905e-02,\n",
       "                      -2.0199e-03, -5.4541e-02,  4.5802e-02, -3.3768e-02, -5.7165e-02,\n",
       "                      -1.9141e-02, -6.1838e-03,  3.7195e-02,  3.4283e-02, -1.3327e-03,\n",
       "                      -1.4604e-02, -9.5513e-03, -2.9084e-02, -3.8677e-02,  5.0552e-02,\n",
       "                       2.5215e-02,  8.8999e-03,  2.3351e-02,  5.2901e-02], device='cuda:0')),\n",
       "             ('conv4.0.weight',\n",
       "              tensor([[[[ 2.5809e-02,  2.3848e-02,  3.5727e-02],\n",
       "                        [ 3.0398e-02,  3.3917e-02,  8.7754e-03],\n",
       "                        [-1.7108e-02, -1.8042e-02,  3.6800e-03]],\n",
       "              \n",
       "                       [[ 1.4340e-02,  3.9650e-02,  3.8773e-02],\n",
       "                        [-2.6526e-02, -5.5746e-04,  2.4038e-04],\n",
       "                        [-2.6275e-02, -3.5849e-02,  1.6821e-03]],\n",
       "              \n",
       "                       [[ 9.7846e-03,  4.2468e-04, -2.4327e-02],\n",
       "                        [-2.5926e-02,  9.6013e-03,  9.2716e-03],\n",
       "                        [-2.4734e-02,  2.7503e-02,  9.4287e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.0183e-02,  1.7563e-02,  2.7877e-02],\n",
       "                        [-6.3946e-03,  2.9981e-02,  3.0045e-02],\n",
       "                        [-3.9798e-02,  8.4463e-04,  3.7229e-02]],\n",
       "              \n",
       "                       [[-1.6740e-02, -2.2392e-02, -2.6093e-02],\n",
       "                        [-3.6951e-02, -3.9396e-02,  1.0890e-03],\n",
       "                        [ 1.6541e-03, -2.2570e-02,  3.5827e-02]],\n",
       "              \n",
       "                       [[-4.0931e-02, -3.3026e-02, -1.9538e-02],\n",
       "                        [ 2.4339e-02, -3.8852e-03,  9.5936e-03],\n",
       "                        [-3.3101e-02, -1.0135e-02, -1.4312e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.5598e-02, -3.7817e-02, -3.2241e-02],\n",
       "                        [-9.5454e-03,  1.4174e-02, -3.3807e-03],\n",
       "                        [-3.5992e-02,  2.9349e-02, -3.0689e-02]],\n",
       "              \n",
       "                       [[ 7.2771e-03, -1.6602e-02,  2.8230e-02],\n",
       "                        [ 4.1364e-03, -3.1173e-02,  2.6130e-02],\n",
       "                        [ 3.2410e-03, -3.0058e-02, -3.5728e-02]],\n",
       "              \n",
       "                       [[-1.0161e-02, -2.1562e-02, -1.2855e-02],\n",
       "                        [ 1.5169e-02,  3.0149e-02,  4.3636e-02],\n",
       "                        [-2.3173e-02,  2.6060e-02, -2.8396e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.4392e-03, -2.4606e-02,  1.6673e-02],\n",
       "                        [ 6.0838e-03,  2.8243e-02,  1.0245e-04],\n",
       "                        [-3.4797e-02,  1.9416e-02, -2.8844e-02]],\n",
       "              \n",
       "                       [[-3.4327e-02, -2.2715e-02, -2.3367e-02],\n",
       "                        [-1.1224e-02, -3.2364e-02,  2.8070e-02],\n",
       "                        [-1.7429e-02,  1.7275e-02,  1.5318e-02]],\n",
       "              \n",
       "                       [[ 1.1969e-02, -1.5767e-02,  2.6104e-02],\n",
       "                        [-1.5944e-02,  3.0022e-02,  2.1374e-02],\n",
       "                        [ 3.8158e-02, -4.3496e-03,  5.0646e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0136e-02,  2.0887e-02,  2.6744e-02],\n",
       "                        [-1.5530e-02, -4.0842e-02,  3.9719e-02],\n",
       "                        [ 2.7219e-02,  4.5224e-02,  1.2042e-02]],\n",
       "              \n",
       "                       [[-2.6027e-02,  3.0804e-02,  5.1434e-03],\n",
       "                        [ 3.7521e-02,  1.7194e-02,  1.1631e-02],\n",
       "                        [ 1.4272e-03, -2.2362e-02,  3.9863e-02]],\n",
       "              \n",
       "                       [[ 2.3987e-02,  1.7314e-02,  2.2109e-03],\n",
       "                        [-1.1669e-02,  3.8213e-02,  3.4808e-03],\n",
       "                        [ 4.1497e-02,  2.6612e-02,  1.1226e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.7529e-03, -1.9270e-02, -1.0369e-02],\n",
       "                        [-3.5951e-02, -4.0216e-02, -1.3231e-02],\n",
       "                        [-2.8203e-02, -2.4525e-02,  3.8537e-02]],\n",
       "              \n",
       "                       [[-1.4454e-02,  3.7270e-02,  3.6466e-02],\n",
       "                        [ 8.0300e-03, -2.6479e-02,  1.6273e-02],\n",
       "                        [-2.9857e-02,  2.0617e-02,  3.4865e-02]],\n",
       "              \n",
       "                       [[-1.1416e-02,  2.8474e-02,  2.6563e-02],\n",
       "                        [-1.6615e-03, -2.8266e-02, -2.1696e-02],\n",
       "                        [ 3.0880e-02,  3.3819e-02,  3.7463e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7593e-02, -4.1397e-02, -2.2671e-02],\n",
       "                        [ 3.2218e-02, -1.1668e-02, -4.1297e-02],\n",
       "                        [-1.1976e-02, -1.7041e-02,  2.1929e-02]],\n",
       "              \n",
       "                       [[ 1.9176e-03,  1.8358e-02, -2.8461e-02],\n",
       "                        [-4.0293e-02, -6.9428e-03,  1.7913e-02],\n",
       "                        [ 1.1892e-02, -2.5119e-02,  2.9897e-02]],\n",
       "              \n",
       "                       [[ 2.8816e-02, -4.4713e-03, -1.1467e-05],\n",
       "                        [-3.3511e-02, -3.8648e-02,  4.1955e-02],\n",
       "                        [-1.6820e-02, -1.8561e-02,  3.2505e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.1574e-02,  2.1262e-02, -5.4883e-03],\n",
       "                        [-7.6528e-03,  2.6241e-02, -5.2473e-04],\n",
       "                        [ 1.0323e-02,  2.5276e-02, -2.7109e-02]],\n",
       "              \n",
       "                       [[ 3.8754e-02, -6.9047e-04,  4.1347e-02],\n",
       "                        [-2.7742e-02, -3.2517e-02,  2.7592e-02],\n",
       "                        [-1.0446e-02, -2.4551e-02, -3.6496e-02]],\n",
       "              \n",
       "                       [[-2.4402e-02, -3.5444e-03, -3.8236e-03],\n",
       "                        [ 3.5752e-02, -2.7336e-02, -1.3611e-02],\n",
       "                        [-3.6173e-02, -3.8729e-03,  8.6307e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2727e-02,  1.1596e-02,  1.8996e-02],\n",
       "                        [-1.8374e-02,  1.4613e-02,  2.0655e-03],\n",
       "                        [ 1.8768e-02, -2.5944e-02, -2.8664e-02]],\n",
       "              \n",
       "                       [[-2.6338e-03,  1.7009e-02,  1.5044e-02],\n",
       "                        [-2.4914e-02,  3.1394e-02, -3.8494e-02],\n",
       "                        [ 5.0949e-03, -9.0436e-04,  3.2831e-02]],\n",
       "              \n",
       "                       [[ 1.2731e-02, -3.8005e-02,  3.2594e-02],\n",
       "                        [ 2.6277e-02, -3.5072e-02, -5.9566e-03],\n",
       "                        [-1.6015e-02, -3.9370e-02, -8.0110e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.7449e-02, -1.3621e-02,  1.3276e-02],\n",
       "                        [ 1.5863e-02, -1.6392e-02,  2.9673e-02],\n",
       "                        [-1.2410e-02, -2.5524e-02, -3.7957e-02]],\n",
       "              \n",
       "                       [[ 1.9143e-02,  2.7994e-03,  2.9043e-02],\n",
       "                        [ 3.3674e-02, -3.5990e-03, -4.0925e-02],\n",
       "                        [ 5.6804e-04, -1.7202e-03,  3.9717e-02]],\n",
       "              \n",
       "                       [[-1.4846e-02,  2.6197e-02, -2.3431e-02],\n",
       "                        [-3.9467e-02, -2.8743e-02, -3.2817e-02],\n",
       "                        [ 3.8692e-02,  4.1144e-02, -3.4100e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4138e-02, -9.5975e-03, -3.9322e-02],\n",
       "                        [-3.2240e-02,  1.2521e-02, -3.2691e-02],\n",
       "                        [-1.6872e-02,  3.4374e-02,  3.6240e-02]],\n",
       "              \n",
       "                       [[ 3.3891e-02,  2.8010e-02,  3.6644e-02],\n",
       "                        [ 1.6347e-02,  5.5346e-03,  1.7136e-02],\n",
       "                        [ 2.1086e-03, -1.9004e-02, -2.4064e-02]],\n",
       "              \n",
       "                       [[ 3.8821e-03, -2.2547e-02,  3.7480e-02],\n",
       "                        [ 3.4514e-02, -1.6076e-02,  3.7595e-02],\n",
       "                        [ 3.5980e-02,  3.2488e-02,  7.2953e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0341e-03,  2.8213e-02,  3.5352e-02],\n",
       "                        [ 1.2664e-02,  1.8745e-02,  2.2729e-02],\n",
       "                        [-3.7189e-02, -3.2741e-02, -1.7867e-02]],\n",
       "              \n",
       "                       [[-2.7452e-03, -1.3097e-02,  2.4966e-02],\n",
       "                        [ 2.8546e-02,  2.1314e-02,  2.0794e-02],\n",
       "                        [ 1.8775e-02,  1.9693e-02, -1.8786e-02]],\n",
       "              \n",
       "                       [[ 2.8044e-02, -7.7736e-03, -2.9144e-02],\n",
       "                        [-4.0599e-02, -2.0633e-02, -1.5483e-02],\n",
       "                        [-4.0217e-02,  3.7837e-03, -6.4904e-03]]]], device='cuda:0')),\n",
       "             ('conv4.0.bias',\n",
       "              tensor([ 0.0255,  0.0224,  0.0102, -0.0317, -0.0034,  0.0245, -0.0172,  0.0240,\n",
       "                       0.0034, -0.0280, -0.0177, -0.0110, -0.0409,  0.0290, -0.0257,  0.0286,\n",
       "                      -0.0097, -0.0359, -0.0337,  0.0120,  0.0019,  0.0371,  0.0164, -0.0052,\n",
       "                      -0.0091, -0.0276, -0.0065,  0.0324,  0.0363, -0.0078,  0.0237, -0.0154,\n",
       "                       0.0322,  0.0273, -0.0295, -0.0281, -0.0245,  0.0026, -0.0036, -0.0286,\n",
       "                      -0.0145, -0.0322,  0.0320, -0.0394,  0.0084, -0.0358,  0.0032, -0.0317,\n",
       "                       0.0236,  0.0399,  0.0050, -0.0069,  0.0168,  0.0118,  0.0053,  0.0320,\n",
       "                       0.0033, -0.0389,  0.0204,  0.0021,  0.0307,  0.0423, -0.0302,  0.0368,\n",
       "                      -0.0324, -0.0218, -0.0274, -0.0372,  0.0148, -0.0094,  0.0205, -0.0362,\n",
       "                       0.0046, -0.0296,  0.0078,  0.0128,  0.0315,  0.0214,  0.0327, -0.0416,\n",
       "                      -0.0054, -0.0314,  0.0322,  0.0230,  0.0169, -0.0127,  0.0056,  0.0028,\n",
       "                       0.0139,  0.0179, -0.0162,  0.0191, -0.0010,  0.0266, -0.0374, -0.0297,\n",
       "                      -0.0230,  0.0421, -0.0168,  0.0138,  0.0213,  0.0409,  0.0025, -0.0214,\n",
       "                       0.0078, -0.0327,  0.0177,  0.0274,  0.0264,  0.0072,  0.0040, -0.0391,\n",
       "                       0.0140, -0.0320, -0.0350,  0.0153, -0.0118, -0.0258, -0.0360,  0.0068,\n",
       "                      -0.0307,  0.0201,  0.0178,  0.0289, -0.0257, -0.0220, -0.0094, -0.0364],\n",
       "                     device='cuda:0')),\n",
       "             ('linear.weight',\n",
       "              tensor([[ 0.0083,  0.0105, -0.0061,  ..., -0.0085,  0.0123, -0.0050],\n",
       "                      [-0.0033, -0.0142,  0.0082,  ...,  0.0122, -0.0024, -0.0095],\n",
       "                      [-0.0057,  0.0141, -0.0164,  ...,  0.0104, -0.0109,  0.0019],\n",
       "                      ...,\n",
       "                      [-0.0150,  0.0042, -0.0128,  ...,  0.0043,  0.0079, -0.0134],\n",
       "                      [-0.0104,  0.0066,  0.0003,  ...,  0.0033, -0.0112, -0.0204],\n",
       "                      [-0.0259, -0.0217, -0.0004,  ...,  0.0045,  0.0045, -0.0027]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0131, -0.0151, -0.0016, -0.0181, -0.0188, -0.0171, -0.0002, -0.0207,\n",
       "                       0.0231, -0.0031], device='cuda:0'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a432ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model stored\n"
     ]
    }
   ],
   "source": [
    "torch.save(loaded_model_0.state_dict(), \"loaded_cnnnet.pth\")\n",
    "print(\"Model stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4960a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe025602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f795764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
